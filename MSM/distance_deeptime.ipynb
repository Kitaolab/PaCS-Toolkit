{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the notebook\n",
    "### Preparation\n",
    "- Import libralies, set necessary variables, etc.\n",
    "\n",
    "### Check the progress of PaCS-MD\n",
    "- You can check the progress of PaCS-MD\n",
    "  - Plot features \n",
    "  - **this functionality is available even before you finish PaCS-MD**\n",
    "\n",
    "### 1D MSM \n",
    "- Build MSM on the inter-COM distance $d$ (1-dimension) for each trial respectively.\n",
    "- 1D: Clustering\n",
    "- 1D: Plot Histogram of $d$\n",
    "- 1D: Plot Inertias\n",
    "- 1D: Build MSM\n",
    "- 1D: Plot ITS using distance\n",
    "- 1D: FEL along inter-COM distance $d$\n",
    "- 1D: Binding Free Energy\n",
    "\n",
    "### 3D-MSM\n",
    "- Build MSM on the inter-COM vector after fitting $\\bm{d}$ (3-dimension) at once for all trials.\n",
    "- 3D: Clustering\n",
    "- 3D: Plot Histogram of $d$\n",
    "- 3D: Plot Inertias\n",
    "- 3D: Build MSM\n",
    "- 3D: Plot ITS using distance\n",
    "- 3D: FEL along $d$\n",
    "- 3D: Binding Free Energy\n",
    "- 3D: $k_{on}, k_{off}$\n",
    "- 3D: FEL on the 2D plane\n",
    "\n",
    "### Reference\n",
    "- List of reference papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import subprocess as sb\n",
    "import sys, logging\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deeptime\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.interpolate import CubicSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input the following variables\n",
    "refer to the MSM/README.md for the details of each variable\n",
    "an input parameter sample is available at the cell below\n",
    "\"\"\"\n",
    "\n",
    "# input the following variables\n",
    "n_trial_for_calc      : List[int] = # enter here                    # list of indices of trials\n",
    "trial_root_directory  : str       = \"/path/to/trial\"                # directory path containing trial directories\n",
    "feature_1d_directory  : str       = \"/path/to/comdist-genfeature\"   # directory path containing comdist (created by pacs gengeature comdist)\n",
    "feature_3d_directory  : str       = \"/path/to/comvec-genfeature\"    # directory path containing comvec (created by pacs gengeature comvec)\n",
    "output_directory      : str       = \"./out_distnb\"                  # output directory path created by this notebook\n",
    "show_picture          : bool      = True                            # whether to show pictures in this notebook, select from [True, False]\n",
    "T                     : float     = # enter here                    # [K], temperature in your system\n",
    "dt                    : int       = # enter here                    # [ps], time interval of saved trajectory in PaCS-MD\n",
    "n_clusters_for_try_1d : List[int] = [20, 30, 40, 50, 60, 70, 80]    # n_clusters list for plotting ITS\n",
    "lags_for_try_1d       : List[int] = [i for i in range(1, 51)]       # lag time [steps] list for plotting ITS (1 step = 1 interval in the trajectory)\n",
    "n_clusters_for_try_3d : List[int] = [100, 200, 300, 400, 500]       # n_clusters list for plotting ITS\n",
    "lags_for_try_3d       : List[int] = [i for i in range(1, 51)]       # lag time [steps] list for plotting ITS (1 step = 1 interval in the trajectory)\n",
    "cutoff                : float     = 1000 # decrease in need         # [nm], cutoff for inter-COM distance to decide whether the replica is used for MSM.\n",
    "nbins                 : int       = # enter here                    # the number of bins when plotting.\n",
    "cmap                  : ListedColormap = mpl.colormaps.get_cmap('tab20')   # color map for plotting FEL of each trial\n",
    "do_volume_correction  : bool      = True                            # whether to perform volume correction, select from [True, False]\n",
    "num_of_ligand         : int       = 1                               # the number of ligands in your system\n",
    "box_size              : float     = # enter here                    # [nm^3], box size for volume ligand concentration used for koff calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> sample parameters (click to expand) </summary>\n",
    "\n",
    "```python\n",
    "# input the following variables\n",
    "n_trial_for_calc      : List[int] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] # list of indices of trials\n",
    "trial_root_directory  : str       = \"/path/to/trial\"                # directory path containing trial directories\n",
    "feature_1d_directory  : str       = \"/path/to/comdist-genfeature\"   # directory path containing comdist (created by pacs gengeature comdist)\n",
    "feature_3d_directory  : str       = \"/path/to/comvec-genfeature\"    # directory path containing comvec (created by pacs gengeature comvec)\n",
    "output_directory      : str       = \"./out_distnb\"                  # output directory path created by this notebook\n",
    "show_picture          : bool      = True                            # whether to show pictures in this notebook, select from [True, False]\n",
    "T                     : float     = 303.15                          # [K], temperature in your system\n",
    "dt                    : int       = 1                               # [ps], dt in your system\n",
    "n_clusters_for_try_1d : List[int] = [20, 30, 40, 50, 60, 70, 80]    # n_clusters list for plotting ITS\n",
    "lags_for_try_1d       : List[int] = [i for i in range(1, 51)]       # lag time(frame) list for plotting ITS\n",
    "n_clusters_for_try_3d : List[int] = [100, 200, 300, 400, 500]       # n_clusters list for plotting ITS\n",
    "lags_for_try_3d       : List[int] = [i for i in range(1, 51)]       # lag time(frame) list for plotting ITS\n",
    "cutoff                : float     = 6.5                             # [nm], cutoff for inter-COM distance to decide whether the replica is used for MSM.\n",
    "nbins                 : int       = 30                              # the number of bins when plotting.\n",
    "cmap                  : ListedColormap = mpl.cm.get_cmap('tab20')   # color map for plotting FEL of each trial\n",
    "do_volume_correction  : bool      = True                            # whether to perform volume correction, select from [True, False]\n",
    "num_of_ligand         : int       = 1                               # the number of ligands in your system\n",
    "box_size              : float     = 9.31680 * 9.31680 * 16.54044    # [nm^3], box size for volume ligand concentration used for koff calculation\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the number of trials is greater than 20, you need to add a color map in cmap to plot for each trial\n",
    "# here is an example of concatenating tab20 and tab20b into another color map, which you will use in plotting FEL\n",
    "\n",
    "# get tab20 and tab20b color map\n",
    "cmap_tab20 = mpl.colormaps.get_cmap('tab20')\n",
    "cmap_tab20b = mpl.colormaps.get_cmap('tab20b')\n",
    "\n",
    "# create new color map by concatenating tab20 and tab20b\n",
    "colors_tab20_tab20b = np.vstack((cmap_tab20(np.linspace(0, 1, 20)),\n",
    "                                 cmap_tab20b(np.linspace(0, 1, 20))))\n",
    "cmap_tab20_tab20b = ListedColormap(colors_tab20_tab20b)\n",
    "\n",
    "# set new color map to \"cmap\" variable\n",
    "cmap: ListedColormap = cmap_tab20_tab20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color list consistent with VMD(VIsual Molecular Dynamics) color list\n",
    "# If you want to use VMD color list to compare with the dissociation pathways by VMD, you can use the following color list\n",
    "# 32 colors\n",
    "rgb_list = [ \n",
    "    [1, 0, 0],          # 1: red\n",
    "    [0.35, 0.35, 0.35], # 2: gray\n",
    "    [1, 0.5, 0],        # 3: orange\n",
    "    [1, 1, 0],          # 4: yellow\n",
    "    [0.5, 0.5, 0.2],    # 5: tan\n",
    "    [0.6, 0.6, 0.6],    # 6: silver\n",
    "    [0, 1, 0],          # 7: green\n",
    "    [1, 1, 1],          # 8: white\n",
    "    [1, 0.6, 0.6],      # 9: pink\n",
    "    [0.25, 0.75, 0.75], # 10: cyan\n",
    "    [0.65, 0, 0.65],    # 11: purple\n",
    "    [0.5, 0.9, 0.4],    # 12: lime\n",
    "    [0.9, 0.4, 0.7],    # 13: mauve\n",
    "    [0.5, 0.3, 0],      # 14: orche\n",
    "    [0.5, 0.5, 0.75],   # 15: iceblue\n",
    "    [0, 0, 0],          # 16: black\n",
    "    [0.88, 0.97, 0.02], # 17: yellow2\n",
    "    [0.55, 0.9, 0.02],  # 18: yellow3\n",
    "    [0, 0.9, 0.04],     # 19: green2\n",
    "    [0, 0.9, 0.5],      # 20: green3\n",
    "    [0, 0.88, 1],       # 21: cyan2\n",
    "    [0, 0.76, 1],       # 22: cyan3\n",
    "    [0.02, 0.38, 0.67], # 23: blue2\n",
    "    [0.01, 0.04, 0.93], # 24: blue3\n",
    "    [0.27, 0, 0.98],    # 25: violet\n",
    "    [0.45, 0, 0.9],     # 26: violet2\n",
    "    [0.9, 0, 0.9],      # 27: magenta\n",
    "    [1, 0, 0.66],       # 28: magenta2\n",
    "    [0.98, 0, 0.23],    # 29: red2\n",
    "    [0.81,0, 0],        # 30: red3\n",
    "    [0.89, 0.35, 0],    # 31: orange2\n",
    "    [0.96, 0.72, 0],    # 32: orange3\n",
    "]\n",
    "\n",
    "hex_colors = [mpl.colors.to_hex(rgb) for rgb in rgb_list]\n",
    "vmd_cmap = ListedColormap(hex_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the logger object and log file\n",
    "logger = logging.getLogger(__name__)\n",
    "log_file = Path(output_directory) / \"logs/distance_deeptime.log\"\n",
    "log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# define the formatter\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s [%(levelname)s] (%(funcName)s) : %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# file handler for logging (save to log file)\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# stream handler for logging (print to console)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# logging the error to both console and file\n",
    "# a bit dirty way to handle unexpected errors, but it works\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        # KeyboardInterrupt is not an error\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "    logger.error(\n",
    "        \"An unhandled exception occurred.\", \n",
    "        exc_info=(exc_type, exc_value, exc_traceback)\n",
    "    )\n",
    "\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class Parameters():\n",
    "    T                       : float    \n",
    "    trial_root_directory    : str \n",
    "    n_trial_for_calc        : List[int]\n",
    "    feature_1d_directory    : str\n",
    "    feature_3d_directory    : str\n",
    "    output_directory        : str\n",
    "    show_picture            : bool\n",
    "    dt                      : int\n",
    "    n_clusters_for_try_1d   : List[int]\n",
    "    lags_for_try_1d         : List[int]\n",
    "    n_clusters_for_try_3d   : List[int]\n",
    "    lags_for_try_3d         : List[int]\n",
    "    cutoff                  : float\n",
    "    nbins                   : int\n",
    "    do_volume_correction    : bool\n",
    "    num_of_ligand           : int\n",
    "    box_size                : float\n",
    "    cmap                    : ListedColormap\n",
    "    logger                  : logging.Logger = logger\n",
    "    _RT                     : float     = - 8.314 * T / 1000 / 4.184  # constant for calculating free energy [kcal/mol/T]\n",
    "    ligand_concentration    : float     = num_of_ligand / box_size * (10 / 6.022)  # [mol/L], ligand concentration used for koff calculation\n",
    "    \n",
    "# parameters\n",
    "params = Parameters(\n",
    "    T=T,\n",
    "    trial_root_directory=trial_root_directory,\n",
    "    n_trial_for_calc=n_trial_for_calc,\n",
    "    feature_1d_directory=feature_1d_directory,\n",
    "    feature_3d_directory=feature_3d_directory,\n",
    "    output_directory=output_directory,\n",
    "    show_picture=show_picture,\n",
    "    dt=dt,\n",
    "    n_clusters_for_try_1d=n_clusters_for_try_1d,\n",
    "    lags_for_try_1d=lags_for_try_1d,\n",
    "    n_clusters_for_try_3d=n_clusters_for_try_3d,\n",
    "    lags_for_try_3d=lags_for_try_3d,\n",
    "    cutoff=cutoff,\n",
    "    nbins=nbins,\n",
    "    do_volume_correction=do_volume_correction,\n",
    "    num_of_ligand=num_of_ligand,\n",
    "    box_size=box_size,\n",
    "    cmap=cmap,  # use vmd_cmap if you want to use VMD color list\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# create output directory\n",
    "Path(params.output_directory).mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{params.output_directory}/images\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{params.output_directory}/cluster_objs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{params.output_directory}/MSM_objs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{params.output_directory}/Count_objs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{params.output_directory}/result_csvs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{params.output_directory}/logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "if not params.show_picture:\n",
    "    mpl.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature\n",
    "- plot top-rank feature for each cycle\n",
    "- can be executed even before the PaCS-MD calculation has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(\n",
    "    params: Parameters,\n",
    "    threshold: float\n",
    ") -> None:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        # if not Path(f\"{params.trial_root_directory}/t{trial:03}\").exists():\n",
    "        if not Path(f\"{params.feature_1d_directory}/t{trial:03}c001r001.npy\").exists():\n",
    "            params.logger.info(f\"trial{trial:03} was not found\")\n",
    "            continue\n",
    "        else:\n",
    "            res = sb.run(\n",
    "                f\"head -n 1 {params.trial_root_directory}/trial{trial:03}/cycle*/summary/cv_ranked.log | grep frame | awk '{{print $6}}'\",\n",
    "                shell=True, \n",
    "                text=True, \n",
    "                capture_output=True\n",
    "            )\n",
    "            if res.returncode != 0:\n",
    "                params.logger.error(f\"error occurred at head command\")\n",
    "                sys.exit(1)\n",
    "            y_list = [float(line) for line in res.stdout.strip().split(\"\\n\")]\n",
    "            # y_list = []\n",
    "            # print(f\"trial{trial:03} is plotting\")\n",
    "            # for cycle in range(1, 210, 1):\n",
    "            #     cycle_list = []\n",
    "            #     for rep_path in Path(f\"{params.feature_1d_directory}\").glob(f\"t{trial:03}c{cycle:03}r*.npy\"):\n",
    "            #         tmp_y_list = np.load(rep_path)\n",
    "            #         cycle_list.append(max(tmp_y_list))\n",
    "            #     else:\n",
    "            #         if len(cycle_list) == 0:\n",
    "            #             break\n",
    "            #     y_list.append(max(cycle_list))\n",
    "            plt.plot(y_list, label=f\"trial{trial:03}\", color=params.cmap(trial))\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label='unbound state')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"Inter-COM [nm]\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    # plt.xlim(0, 200)\n",
    "    # plt.ylim(0, 10)\n",
    "    plt.savefig(\n",
    "        f\"{params.output_directory}/images/cycle_feature.png\", \n",
    "        bbox_inches='tight',\n",
    "        pad_inches=0.1,\n",
    "        dpi=300\n",
    "    )\n",
    "    if params.show_picture:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# Set the threshold to the lower bound of the unbound state for dissociation simulation\n",
    "\n",
    "plot_feature(params=params, threshold=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Clustering\n",
    "- Perform k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_1d(\n",
    "        params: Parameters,\n",
    "        max_iter: int = 2000,\n",
    "):  \n",
    "    for trial in params.n_trial_for_calc:\n",
    "        # check if clustering results already exists\n",
    "        if exist_all_clustering(params, trial):\n",
    "            params.logger.info(f\"All clustering results for trial{trial:03} already exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # loads\n",
    "        features = []\n",
    "        feature_files_trial = list(\n",
    "            Path(f\"{params.feature_1d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "        )\n",
    "        feature_files_trial.sort()\n",
    "        for rep_path in feature_files_trial:\n",
    "            features_per_rep = np.load(rep_path)\n",
    "            max_distance = np.max(features_per_rep)\n",
    "            if max_distance > params.cutoff:\n",
    "                continue\n",
    "            else:\n",
    "                features.append(features_per_rep)\n",
    "        \n",
    "        for n_clusters in params.n_clusters_for_try_1d:\n",
    "            clustering_result_pkl = f\"{params.output_directory}/cluster_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "            if Path(clustering_result_pkl).exists():\n",
    "                params.logger.info(\n",
    "                    f\"Clustering result for trial{trial:03} n_clusters={n_clusters} cutoff={params.cutoff} already exists. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            # clustering\n",
    "            params.logger.info(f\"Starting clustering for trial{trial:03} n_clusters={n_clusters} cutoff={params.cutoff}\")\n",
    "            estimator = deeptime.clustering.KMeans(\n",
    "                n_clusters=n_clusters, \n",
    "                max_iter=max_iter,\n",
    "                metric=\"euclidean\",\n",
    "                tolerance=1e-05,\n",
    "                init_strategy=\"kmeans++\",\n",
    "                fixed_seed=False,\n",
    "                n_jobs=None, \n",
    "                initial_centers=None,\n",
    "                progress=None\n",
    "            )\n",
    "            estimator.fit(np.concatenate(features))\n",
    "            clustering_trjs = []\n",
    "            for features_per_rep in features:\n",
    "                clustering_trjs.append(estimator.transform(features_per_rep))\n",
    "            clustering_model = estimator.fetch_model()\n",
    "\n",
    "            # save result \n",
    "            save_dict = {\n",
    "                \"trjs\": clustering_trjs,\n",
    "                \"model\": clustering_model\n",
    "            }\n",
    "            with open(clustering_result_pkl, \"wb\") as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "            \n",
    "            # create convergence figure\n",
    "            inertias = clustering_model.inertias\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.plot(inertias)\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"inertia\")\n",
    "            plt.title(f\"trial{trial:03} n_clusters{n_clusters} clustering\")\n",
    "            plt.savefig(\n",
    "                f\"{params.output_directory}/images/clustering_converge_1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.png\", \n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.1,   \n",
    "                dpi=300\n",
    "            )\n",
    "            if params.show_picture:\n",
    "                plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "\n",
    "            params.logger.info(f\"Finished clustering for trial{trial:03} n_clusters={n_clusters} cutoff={params.cutoff}\")\n",
    "        \n",
    "def exist_all_clustering(params:Parameters, trial: int) -> bool:\n",
    "    for n_clusters in params.n_clusters_for_try_1d:\n",
    "        clustering_result_pkl = f\"{params.output_directory}/cluster_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if not Path(clustering_result_pkl).exists():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "cluster_1d(\n",
    "    params=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Plot Histgram of $d$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_1d_per_trial(\n",
    "    params: Parameters,\n",
    "    trial: int,\n",
    "    n_clusters: int,\n",
    ") -> None:\n",
    "    # load clustering result\n",
    "    params.logger.info(f\"Starting histogram plotting for trial{trial:03} n_clusters{n_clusters}\")\n",
    "    clustering_result_pkl = f\"{params.output_directory}/cluster_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "    if not Path(clustering_result_pkl).exists():\n",
    "        params.logger.info(\n",
    "            f\"Clustering obj for trial{trial:03} n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping. First, run clustering.\"\n",
    "        )\n",
    "        return\n",
    "    with open(clustering_result_pkl, \"rb\") as f:\n",
    "        save_dict = pickle.load(f)\n",
    "        clustering_model = save_dict[\"model\"]\n",
    "        center_d = clustering_model.cluster_centers\n",
    "\n",
    "    # load features\n",
    "    features = []\n",
    "    feature_files_trial = list(\n",
    "        Path(f\"{params.feature_1d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "    )\n",
    "    feature_files_trial.sort()\n",
    "    for rep_path in feature_files_trial:\n",
    "        features_per_rep = np.load(rep_path)\n",
    "        max_distance = np.max(features_per_rep)\n",
    "        if max_distance > params.cutoff:\n",
    "            continue\n",
    "        else:\n",
    "            features.append(features_per_rep)\n",
    "    \n",
    "    # create figure\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.vlines(\n",
    "        center_d, \n",
    "        0, \n",
    "        10**10, \n",
    "        \"tab:red\", \n",
    "        linewidth=0.5,\n",
    "        label=\"cluster center\"\n",
    "    )\n",
    "    plt.vlines( # comment out if you don't need cutoff line (default: cutoff=1000)\n",
    "        params.cutoff, \n",
    "        0, \n",
    "        10**10, \n",
    "        \"magenta\", \n",
    "        linewidth=0.5,\n",
    "        label=\"cutoff\"\n",
    "    )\n",
    "    plt.hist(\n",
    "        np.concatenate(features), \n",
    "        bins=100, \n",
    "        alpha=0.5\n",
    "    )\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"$d$ [nm]\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"trial{trial:03} Inter-COM distance distribution\")\n",
    "    # put legend on right hand\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.ylim(1, 10**6)\n",
    "    plt.savefig(\n",
    "        f\"{params.output_directory}/images/hist_1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.png\", \n",
    "        bbox_inches='tight',\n",
    "        pad_inches=0.1,   \n",
    "        dpi=300\n",
    "    )\n",
    "    if params.show_picture:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    params.logger.info(f\"Finished histogram plotting for trial{trial:03} n_clusters{n_clusters}\")\n",
    "\n",
    "def plot_hist_1d(\n",
    "    params: Parameters,\n",
    "    n_clusters: int,\n",
    ") -> None:\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        plot_hist_1d_per_trial(\n",
    "            params=params, \n",
    "            trial=trial,\n",
    "            n_clusters=n_clusters,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "for n_clusters in params.n_clusters_for_try_1d:\n",
    "    plot_hist_1d(\n",
    "        params=params, \n",
    "        n_clusters=n_clusters\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Plot inertias\n",
    "- plot inertias along n_clusters\n",
    "- can be used to decide appropriate n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inertia_1d(params: Parameters):\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        params.logger.info(f\"Starting plotting inertia for trial{trial:03}\")\n",
    "\n",
    "        # load\n",
    "        n_clusters_list = []\n",
    "        inertias = []\n",
    "        for n_clusters in params.n_clusters_for_try_1d:\n",
    "            clustering_result = f\"{params.output_directory}/cluster_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "            if not Path(clustering_result).exists():\n",
    "                params.logger.info(\n",
    "                    f\"Clustering obj for trial{trial:03} n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping. First, run clustering.\"\n",
    "                )\n",
    "                continue\n",
    "            with open(clustering_result, \"rb\") as f:\n",
    "                save_dict = pickle.load(f)\n",
    "                clustering_model = save_dict[\"model\"]\n",
    "            n_clusters_list.append(n_clusters)\n",
    "            inertias.append(clustering_model.inertia)\n",
    "\n",
    "        # check \n",
    "        if len(n_clusters_list) == 0:\n",
    "            params.logger.info(f\"Clustering result not found for trial{trial:03}. No need to plot. Skipping.\")\n",
    "            return\n",
    "        elif len(n_clusters_list) == 1:\n",
    "            params.logger.info(f\"Only one clustering result found for trial{trial:03}. Cannot plot. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # plot\n",
    "        plt.figure(figsize=(3, 2))\n",
    "        plt.plot(n_clusters_list, inertias, marker=\".\")\n",
    "        plt.xlabel(\"number of clusters\")\n",
    "        plt.ylabel(\"inertia\")\n",
    "        plt.title(f\"trial{trial:03}\")\n",
    "        plt.savefig(\n",
    "            f\"{params.output_directory}/images/inertia_1d_trial{trial:03}_cut{params.cutoff}.png\", \n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            dpi=300\n",
    "        )\n",
    "        if params.show_picture:\n",
    "            plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        params.logger.info(f\"Finished plotting inertia for trial{trial:03}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "plot_inertia_1d(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Build MSM \n",
    "- build MSM for each trial and n_clusters as specified at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_msm_1d(\n",
    "        params: Parameters,\n",
    ") -> None:\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        for n_clusters in params.n_clusters_for_try_1d:\n",
    "            # load or initialize msm result\n",
    "            msm_result_pkl = f\"{params.output_directory}/MSM_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "            if not Path(msm_result_pkl).exists():\n",
    "                msm_result = dict()\n",
    "            else:\n",
    "                with open(msm_result_pkl, \"rb\") as f:\n",
    "                    msm_result = pickle.load(f)\n",
    "            \n",
    "            # load or itinitialize count model result\n",
    "            count_result_pkl = f\"{params.output_directory}/Count_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "            if not Path(count_result_pkl).exists():\n",
    "                count_result = dict()\n",
    "            else:\n",
    "                with open(count_result_pkl, \"rb\") as f:\n",
    "                    count_result = pickle.load(f)\n",
    "            \n",
    "            # check if all clustering results exist\n",
    "            clustering_result_pkl = f\"{params.output_directory}/cluster_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "            if not Path(clustering_result_pkl).exists():\n",
    "                params.logger.info(\n",
    "                    f\"Clustering obj for trial{trial:03} n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping. First, run clustering.\"\n",
    "                )\n",
    "                continue\n",
    "            with open(clustering_result_pkl, \"rb\") as f:\n",
    "                save_dict = pickle.load(f)\n",
    "                trjs = save_dict[\"trjs\"]\n",
    "                clustering_model = save_dict[\"model\"]\n",
    "            \n",
    "            # build msm for all lags in params.lags_for_try_1d\n",
    "            for lag in params.lags_for_try_1d:\n",
    "                # skip if already exits\n",
    "                if lag in msm_result.keys():\n",
    "                    params.logger.info(\n",
    "                        f\"MSM result for trial{trial:03} n_clusters={n_clusters} lag={lag} already exists. Skipping.\"\n",
    "                    )\n",
    "                    continue\n",
    "                \n",
    "                params.logger.info(f\"Starting MSM for trial{trial:03} n_clusters={n_clusters} lag={lag}\")\n",
    "                # build count matrix\n",
    "                count_estimator = deeptime.markov.TransitionCountEstimator(\n",
    "                    lagtime=lag, \n",
    "                    count_mode=\"sliding\",\n",
    "                    n_states=None,\n",
    "                    sparse=False\n",
    "                )\n",
    "                count_model = count_estimator.fit(trjs).fetch_model()\n",
    "\n",
    "                # build msm\n",
    "                msm_estimator = deeptime.markov.msm.MaximumLikelihoodMSM(\n",
    "                    reversible=True,\n",
    "                    stationary_distribution_constraint=None,\n",
    "                    sparse=False,\n",
    "                    allow_disconnected=False,\n",
    "                    maxiter=1000000,\n",
    "                    maxerr=1e-08,\n",
    "                    connectivity_threshold=0,\n",
    "                    transition_matrix_tolerance=1e-06,\n",
    "                    lagtime=None,\n",
    "                    use_lcc=False,\n",
    "                )\n",
    "                try:\n",
    "                    msm_model = msm_estimator.fit(count_model).fetch_model()\n",
    "                except Exception as e:\n",
    "                    params.logger.error(f\"Error occurred in building MSM at trial{trial:03} n_clusters={n_clusters} lag={lag}\")\n",
    "                    params.logger.error(f\"error message: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # save msm result\n",
    "                msm_result[lag] = msm_model\n",
    "                count_result[lag] = count_model\n",
    "\n",
    "                with open(msm_result_pkl, \"wb\") as f:\n",
    "                    pickle.dump(msm_result, f)\n",
    "                with open(count_result_pkl, \"wb\") as f:\n",
    "                    pickle.dump(count_result, f)\n",
    "\n",
    "                # save cluster center coordinates and corresponding stationary destribution\n",
    "                n_clusters_in_clustering = len(clustering_model.cluster_centers)\n",
    "                n_clusters_in_msm = msm_model.n_states\n",
    "                cluster_centers = clustering_model.cluster_centers\n",
    "\n",
    "                stationary_distribution = np.zeros(n_clusters_in_clustering,)\n",
    "                largest_connected_set = deeptime.markov.tools.estimation.largest_connected_set(\n",
    "                    count_model.count_matrix,\n",
    "                    directed=True\n",
    "                )\n",
    "                stationary_distribution[largest_connected_set] = msm_model.stationary_distribution\n",
    "\n",
    "                if n_clusters_in_clustering != n_clusters_in_msm:\n",
    "                    params.logger.info(f\"The number of clusters for clustering and msm are different for trial{trial:03} n_clusters={n_clusters} lag={lag}\")\n",
    "                    params.logger.info(f\"Number of clusters for clustering: {n_clusters_in_clustering}\")\n",
    "                    params.logger.info(f\"Number of clusters in the largest connected set: {n_clusters_in_msm}\")\n",
    "                    \n",
    "                center_pi_csv = f\"{params.output_directory}/result_csvs/1d_trial{trial:03}_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "                with open(center_pi_csv, \"w\") as f:\n",
    "                    f.write(\"cluster_center_d,cluster_center_pi\\n\")\n",
    "                    num_clusters = len(cluster_centers)\n",
    "                    for cluster_ind in range(num_clusters):\n",
    "                        center_d = cluster_centers[cluster_ind][0]\n",
    "                        pi = stationary_distribution[cluster_ind]\n",
    "                        f.write(f\"{center_d},{pi}\\n\")\n",
    "                params.logger.info(f\"Finished MSM for trial{trial:03} n_clusters={n_clusters} lag={lag}\")\n",
    "            params.logger.info(f\"Finished MSM for trial{trial:03} n_clusters={n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "build_msm_1d(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Plot ITS using distance\n",
    "- calculate \"Implied Time Scale\" for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_its_1d(\n",
    "    params: Parameters\n",
    ") -> None:\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        for n_clusters in params.n_clusters_for_try_1d:\n",
    "            params.logger.info(f\"Starting plotting ITS for trial{trial:03} n_clusters={n_clusters}\")\n",
    "            # load or initialize msm result\n",
    "            msm_result_pkl = f\"{params.output_directory}/MSM_objs/1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "            if not Path(msm_result_pkl).exists():\n",
    "                msm_result = dict()\n",
    "            else:\n",
    "                with open(msm_result_pkl, \"rb\") as f:\n",
    "                    msm_result = pickle.load(f)\n",
    "            \n",
    "            # accumulate msm models and calculate implied timescales\n",
    "            msm_models = []\n",
    "            for lag in sorted(msm_result.keys()):\n",
    "                msm_models.append(msm_result[lag])\n",
    "            its_data = deeptime.util.validation.implied_timescales(\n",
    "                models=msm_models,\n",
    "                n_its=10, # decrease this number for n_clusters < 11\n",
    "            )\n",
    "\n",
    "            # plot its\n",
    "            plt.figure(figsize=(3, 2))\n",
    "            deeptime.plots.plot_implied_timescales(\n",
    "                its_data,\n",
    "                n_its=10, # decrease this number for n_clusters < 11\n",
    "                process=None,\n",
    "                show_mle=True,\n",
    "                show_sample_mean=True, # ignored\n",
    "                show_sample_confidence=True, # ignored\n",
    "                show_cutoff=True, # ignored\n",
    "                sample_confidence=0.95, # ignored\n",
    "                colors=None,\n",
    "                # ax=None,\n",
    "            )\n",
    "            plt.title(f\"trial{trial:03} n_clusters{n_clusters}\")\n",
    "            plt.xlabel(\"lag time (steps)\") # 1 step = 1 interval in the trajectory\n",
    "            plt.ylabel(\"implied timescale (steps)\") # 1 step = 1 interval in the trajectory\n",
    "            plt.yscale(\"log\")\n",
    "            plt.savefig(\n",
    "                f\"{params.output_directory}/images/its_1d_trial{trial:03}_n_clusters{n_clusters}_cut{params.cutoff}.png\", \n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.1,\n",
    "                dpi=300\n",
    "            )\n",
    "            if params.show_picture:\n",
    "                plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()        \n",
    "            params.logger.info(f\"Finished plotting ITS for trial{trial:03} n_clusters={n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "plot_its_1d(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select n_clusters and lag time to use for later analysis in this notebook.\n",
    "\n",
    "You can select parameters where implied timescales above is converged.\n",
    "\n",
    "The parameters selected here will be subsequently used for plotting FEL, calculating binding free energy in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the default value\n",
    "n_clusters_1d_main = params.n_clusters_for_try_1d\n",
    "lags_1d_main = params.lags_for_try_1d\n",
    "\n",
    "# if you save the time, please change the value as below\n",
    "# n_clusters_1d_main = [50]\n",
    "# lags_1d_main = [30, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: FEL along inter-COM distance $d$\n",
    "\n",
    "- Objective:  \n",
    "  Calculate the free energy landscape (FEL) along the inter-COM distance $d$.\n",
    "\n",
    "- Plotting Options:  \n",
    "  Several options are available for plotting the FEL. \n",
    "\n",
    "  (`linear` is default and recommended)\n",
    "\n",
    "---\n",
    "\n",
    "### When `interpolate_type` is set to `linear` or `cubic`\n",
    "\n",
    "1. Determine the Bin Ranges:\n",
    "\n",
    "   - Let the cluster center of the $l$th cluster in trial $k$ be denoted as $d^{k, l}$.\n",
    "   - Find the overall minimum and maximum among all $d^{k, l}$.\n",
    "   - Divide the range between these minimum and maximum values into `params.n_bins` equal intervals.\n",
    "   - Define:\n",
    "     - The center of each interval as $d_i$.\n",
    "     - The length of each interval as $\\delta d$.\n",
    "   - **Note:** This bin range is applied uniformly across all trials.\n",
    "\n",
    "2. Plot the FEL for Each Trial Using the Bins:\n",
    "\n",
    "   - For the $k$th trial, compute the free energy at $d_i$ as:\n",
    "     $$\n",
    "     \\Delta W^k(d_i) = - RT \\ln \\left( \\sum_{d_i - \\delta d/2 \\le d^{k, l} \\le d_i + \\delta d/2} \\pi_l \\right)\n",
    "     $$\n",
    "     where $ \\pi_l^k $ is the stationary distribution of the $l$th cluster in trial $k$.\n",
    "   - If the summation is empty (i.e., no $d^{k, l}$ falls within the interval), perform interpolation using the surrounding bins (either linear or cubic, based on `interpolate_type`) to obtain $ \\Delta W^k(d_i)$.\n",
    "\n",
    "3. Plot the Averaged FEL with Standard Error:\n",
    "\n",
    "   - Compute the mean and standard error of the energy at each $d_i$, denoted as $ \\Delta W(d_i)$, using the values $ \\Delta W^k(d_i)$ from all trials:\n",
    "     $$\n",
    "     k = 1, 2, \\ldots, n_{\\text{trials}}.\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### When `interpolate_type` is set to `none`\n",
    "\n",
    "1. Plot the FEL for Each Trial Without Binning:\n",
    "\n",
    "   - For the $k$th trial, the energy for the $l$th cluster is calculated as:\n",
    "     $$\n",
    "     \\Delta W_l^k = - RT \\ln \\left( \\frac{\\pi_l}{\\max_j \\pi_j} \\right)\n",
    "     $$\n",
    "\n",
    "2. Determine the Bin Ranges:\n",
    "\n",
    "   - Let the cluster center of the $l$th cluster in trial $k$ be $d^{k, l}$.\n",
    "   - Find the overall minimum and maximum among all $d^{k, l}$.\n",
    "   - Divide the range between these values into `params.n_bins` equal intervals.\n",
    "   - Define:\n",
    "     - The center of each interval as $d_i$.\n",
    "     - The length of each interval as $\\delta d$.\n",
    "   - **Note:** This bin range is applied uniformly across all trials.\n",
    "\n",
    "3. Plot the Averaged FEL with Standard Error:\n",
    "\n",
    "   - For each interval centered at $d_i$, compute the mean and standard error of the energy $ \\Delta W(d_i)$ from all $ \\Delta W_l^k $ values where:\n",
    "     $$\n",
    "     d_i - \\frac{\\delta d}{2} \\le d^{k, l} \\le d_i + \\frac{\\delta d}{2}.\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fel_along_d_1d(\n",
    "    params:Parameters,\n",
    "    n_clusters: int,\n",
    "    lag: int,\n",
    "    interpolate_type: str = \"linear\",\n",
    ") -> None:\n",
    "    # check interpolation type\n",
    "    if interpolate_type not in [\"linear\", \"cubic\", \"none\"]:\n",
    "        params.logger.error(\n",
    "            f\"Interpolation type {interpolate_type} is not supported.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # log\n",
    "    params.logger.info(\n",
    "        f\"Starting plotting FEL along d for n_clusters={n_clusters} lag={lag}\"\n",
    "    )\n",
    "\n",
    "    if interpolate_type == \"none\":\n",
    "        # doesn't interpolate for each trial\n",
    "        plot_data_x_all = []\n",
    "        plot_data_y_all = []\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        for trial in params.n_trial_for_calc:\n",
    "            # load\n",
    "            msm_result_csv = f\"{params.output_directory}/result_csvs/1d_trial{trial:03}_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "            if not Path(msm_result_csv).exists():\n",
    "                params.logger.info(f\"MSM result csv file not found. Skipping. First, run MSM.\")\n",
    "                continue\n",
    "            df = pd.read_csv(msm_result_csv)\n",
    "            cluster_center_d = df[\"cluster_center_d\"].values\n",
    "            stat_dists = df[\"cluster_center_pi\"].values\n",
    "\n",
    "            # select clusters where stationary distribution is not zero\n",
    "            mask = stat_dists > 0\n",
    "            cluster_center_d = cluster_center_d[mask]\n",
    "            stat_dists = stat_dists[mask]\n",
    "\n",
    "            # sort by d\n",
    "            x_args = np.argsort(cluster_center_d)\n",
    "            cluster_center_d = cluster_center_d[x_args]\n",
    "            stat_dists = stat_dists[x_args]\n",
    "\n",
    "            # plot\n",
    "            y_values = params._RT * np.log(stat_dists)\n",
    "            y_values -= np.min(y_values)\n",
    "            plt.plot(\n",
    "                cluster_center_d, \n",
    "                y_values, \n",
    "                label=f\"trial{trial:03}\",\n",
    "                color=params.cmap(trial)\n",
    "            )\n",
    "            plot_data_x_all.extend(cluster_center_d.tolist())\n",
    "            plot_data_y_all.extend(y_values.tolist())\n",
    "        \n",
    "        # define bins\n",
    "        plot_data_x_all = np.array(plot_data_x_all)\n",
    "        plot_data_y_all = np.array(plot_data_y_all)\n",
    "        bins = np.linspace(\n",
    "            start=min(plot_data_x_all), \n",
    "            stop=max(plot_data_x_all), \n",
    "            num=params.nbins\n",
    "        )\n",
    "        bins -= (bins[1] - bins[0]) / 2\n",
    "        bin_centers = bins[:-1] + (bins[1] - bins[0]) / 2\n",
    "        bin_stat_dist_means = np.zeros(len(bins) - 1)\n",
    "        bin_stat_dist_stds = np.zeros(len(bins) - 1)\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_mask = (plot_data_x_all >= bins[i]) & (plot_data_x_all < bins[i+1])\n",
    "            bin_stat_dist_means[i] = np.mean(plot_data_y_all[bin_mask])\n",
    "            bin_stat_dist_stds[i] = np.std(plot_data_y_all[bin_mask])\n",
    "        bin_stat_dist_stes = bin_stat_dist_stds / np.sqrt(len(params.n_trial_for_calc))\n",
    "\n",
    "        # bin_stat_dist_means -= np.min(bin_stat_dist_means)\n",
    "\n",
    "        plt.plot(\n",
    "            bin_centers, \n",
    "            bin_stat_dist_means, \n",
    "            color=\"black\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            bin_centers, \n",
    "            bin_stat_dist_means, \n",
    "            yerr=bin_stat_dist_stes, \n",
    "            fmt=\"o\", \n",
    "            color=\"black\",\n",
    "            label=r\"Mean $\\pm$SE\",\n",
    "            capsize=3,\n",
    "        )\n",
    "\n",
    "        plt.xlim(0, 8)\n",
    "        plt.ylim(0, 20)\n",
    "        plt.title(f\"n_clusters={n_clusters} lag={lag}\")\n",
    "        plt.xlabel(r\"$d$ [nm]\") \n",
    "        plt.ylabel(r\"$\\Delta W$ [kcal/mol]\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.savefig(\n",
    "            f\"{params.output_directory}/images/fel_1d_n_clusters{n_clusters}_lag{lag}_interp_{interpolate_type}.png\",\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            dpi=300\n",
    "        )\n",
    "        if params.show_picture:\n",
    "            plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "    elif interpolate_type in [\"linear\", \"cubic\"]:\n",
    "        # interpolate for each trial\n",
    "        plot_data_x_all = []\n",
    "        plot_data_y_all = []\n",
    "        plt.figure(figsize=(5, 5))\n",
    "\n",
    "        # define bins\n",
    "        center_list = []\n",
    "        for trial in params.n_trial_for_calc:\n",
    "            # load\n",
    "            msm_result_csv = f\"{params.output_directory}/result_csvs/1d_trial{trial:03}_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "            if not Path(msm_result_csv).exists():\n",
    "                params.logger.info(f\"MSM result csv file not found. Skipping. First, run MSM.\")\n",
    "                continue\n",
    "            df = pd.read_csv(msm_result_csv)\n",
    "            cluster_center_d = df[\"cluster_center_d\"].values.tolist()\n",
    "            center_list.extend(cluster_center_d)\n",
    "        bins = np.linspace(\n",
    "            start=min(center_list), \n",
    "            stop=max(center_list), \n",
    "            num=params.nbins\n",
    "        )\n",
    "        bins -= (bins[1] - bins[0]) / 2\n",
    "        bin_centers = bins[:-1] + (bins[1] - bins[0]) / 2\n",
    "        n_intervals = len(bins) - 1\n",
    "        n_trials = len(params.n_trial_for_calc)\n",
    "        bin_stat_dist_trials = np.zeros((n_intervals, n_trials))\n",
    "        y_values_trials = np.zeros((n_intervals, n_trials))\n",
    "\n",
    "        for trial_index, trial in enumerate(params.n_trial_for_calc):\n",
    "            # load\n",
    "            msm_result_csv = f\"{params.output_directory}/result_csvs/1d_trial{trial:03}_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "            if not Path(msm_result_csv).exists():\n",
    "                params.logger.info(f\"MSM result csv file not found. Skipping. First, run MSM.\")\n",
    "                continue\n",
    "            df = pd.read_csv(msm_result_csv)\n",
    "            cluster_center_d = df[\"cluster_center_d\"].values\n",
    "            stat_dists = df[\"cluster_center_pi\"].values\n",
    "\n",
    "            # summation for all intervals\n",
    "            for bin_i in range(n_intervals):\n",
    "                mask = (cluster_center_d >= bins[bin_i]) & (cluster_center_d < bins[bin_i+1])\n",
    "                bin_stat_dist_trials[bin_i, trial_index] = np.sum(stat_dists[mask])\n",
    "            \n",
    "            # interpolate for the missing values\n",
    "            zero_mask = (bin_stat_dist_trials[:, trial_index] == 0)\n",
    "            nonzero_mask = (bin_stat_dist_trials[:, trial_index] > 0)\n",
    "\n",
    "            x_values_nonzero = bin_centers[nonzero_mask]\n",
    "            y_values_nonzero = params._RT * np.log(bin_stat_dist_trials[nonzero_mask, trial_index])            \n",
    "\n",
    "            x_values_zero = bin_centers[zero_mask]\n",
    "\n",
    "            if interpolate_type == \"cubic\":\n",
    "                cs = CubicSpline(\n",
    "                    x_values_nonzero, y_values_nonzero,\n",
    "                    bc_type=\"natural\", extrapolate=True\n",
    "                )\n",
    "                y_values_zero = cs(x_values_zero)\n",
    "            elif interpolate_type == \"linear\":\n",
    "                y_values_zero = np.interp(\n",
    "                    x_values_zero,\n",
    "                    x_values_nonzero,\n",
    "                    y_values_nonzero,\n",
    "                )\n",
    "\n",
    "            n_points_to_interpolate = len(y_values_zero)\n",
    "            params.logger.info(\n",
    "                f\"trial{trial:03} interpolated {n_points_to_interpolate} empty bin(s)\"\n",
    "                f\" out of {len(bin_centers)} bins by {interpolate_type} interpolation\"\n",
    "            )\n",
    "\n",
    "            y_values_trials[zero_mask, trial_index] = y_values_zero\n",
    "            y_values_trials[nonzero_mask, trial_index] = y_values_nonzero\n",
    "\n",
    "            # plot\n",
    "            y_values_trials[:, trial_index] -= np.min(y_values_trials[:, trial_index])\n",
    "            # y_values = y_values_trials[:, trial_index].copy()\n",
    "            # y_values -= np.min(y_values)\n",
    "            plt.plot(\n",
    "                bin_centers,\n",
    "                y_values_trials[:, trial_index],\n",
    "                label=f\"trial{trial:03}\",\n",
    "                color=params.cmap(trial)\n",
    "            )\n",
    "\n",
    "        # skip MSM-failing trials to calculate mean and std\n",
    "        msm_ok_trial_indices = np.where(~np.all(y_values_trials == 0, axis=0))[0]\n",
    "        y_values_trials = y_values_trials[:, msm_ok_trial_indices]\n",
    "        n_trials_ok = len(msm_ok_trial_indices)\n",
    "\n",
    "        # statistics\n",
    "        bin_stat_dist_means = np.mean(y_values_trials, axis=1)\n",
    "        bin_stat_dist_stds = np.std(y_values_trials, axis=1)\n",
    "        bin_stat_dist_stes = bin_stat_dist_stds / np.sqrt(n_trials_ok)\n",
    "\n",
    "        plt.plot(\n",
    "            bin_centers, \n",
    "            bin_stat_dist_means, \n",
    "            color=\"black\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            bin_centers, \n",
    "            bin_stat_dist_means, \n",
    "            yerr=bin_stat_dist_stes, \n",
    "            fmt=\"o\", \n",
    "            color=\"black\",\n",
    "            label=r\"Mean $\\pm$SE\",\n",
    "            capsize=3,\n",
    "        )\n",
    "\n",
    "        plt.xlim(0, 8)\n",
    "        plt.ylim(0, 20)\n",
    "        plt.title(f\"n_clusters={n_clusters} lag={lag}\")\n",
    "        plt.xlabel(r\"$d$ [nm]\") \n",
    "        plt.ylabel(r\"$\\Delta W$ [kcal/mol]\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.savefig(\n",
    "            f\"{params.output_directory}/images/fel_1d_n_clusters{n_clusters}_lag{lag}_interp_{interpolate_type}.png\",\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            dpi=300\n",
    "        )\n",
    "        if params.show_picture:\n",
    "            plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    \n",
    "    else:\n",
    "        params.logger.error(\n",
    "            f\"Interpolation type {interpolate_type} is not supported.\"\n",
    "        )\n",
    "        return\n",
    "        \n",
    "    params.logger.info(\n",
    "        f\"Finished plotting FEL along d for n_clusters={n_clusters} lag={lag}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "for n_clusters in n_clusters_1d_main:\n",
    "    for lag in lags_1d_main:\n",
    "        plot_fel_along_d_1d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag,\n",
    "            interpolate_type=\"none\"\n",
    "        )\n",
    "        plot_fel_along_d_1d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag,\n",
    "        )\n",
    "        plot_fel_along_d_1d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag,\n",
    "            interpolate_type=\"cubic\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the bound/unbound state according to 1d-fel of your system plotted above.\n",
    "\n",
    "If you don't set the definition, you can't calculate the binding free energy.\n",
    "- the meaning of each variable is written in `1D: Binding Free Energy Calculation` section\n",
    "- unit is [nm]\n",
    "- set as lower_bound < lower_unbound <= upper_unbound < upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values are invalid to prevent mistakes. Please modify them to fit your system.\n",
    "\n",
    "# definition for bound-state\n",
    "lower_bound = 0 # 0 is recommended\n",
    "upper_bound = 5\n",
    "\n",
    "# definition for unbound-state\n",
    "lower_unbound = 4\n",
    "upper_unbound = params.cutoff # params.cutoff is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Binding Free Energy\n",
    "- calculate the standard bindng free energy $\\Delta G^o$ based on the result of MSM.\n",
    "- In case of 1D, **$\\Delta G^o$ is calculated trial by trial** to deal with the possibility of different pathway in different trial.\n",
    "  $$\n",
    "  \\Delta G^o = \\Delta G_{PMF} + \\Delta G_v\n",
    "  $$\n",
    "  1. $\\Delta G_{PMF}$ is the binding free energy term.\n",
    "       - free energy difference between the bound state and the unbound state.\n",
    "  2. $\\Delta G_v$ is the volume cocrrection term.\n",
    "       - necessary to standarize the binding binding free energy\n",
    "         - the binding free energy obtained from MD can have an error due to the limitation of simulation box size.\n",
    "       - This volume correction method is based on Buch et.al (2011)[6], Doudou et al[7].\n",
    "- if `do_volume_correction` = `True`,  volume correction term is calculated.\n",
    "- if `do_volume_correction` = `False`,  volume correction term is not included.\n",
    "\n",
    "\n",
    "#### 1. Binding Free Energy Term $\\Delta G_{PMF}$\n",
    "- need the definition of bound state / unbound state.\n",
    "- this notebook simply define these state by **inter-COM distance** $d$.\n",
    "- you can specify up to 4 parameters regarding this definition.\n",
    "  - `lower_bound`: lower bound of bound state [default: 0].\n",
    "  - `upper_bound`: upper bound of bound state.\n",
    "  - `lower_unbound`: lower bound of unbound state.\n",
    "  - `upper_unbound`: upper bound of unbound state. **should be lower than cutoff of MSM**\n",
    "  - decide these terms based on the PMF plot obtained by the functions above\n",
    "- based on the specified parameters, bound/unbound states can be defined as below.\n",
    "  - bound state: area where `lower_bound` < $d$ < `upper_bound`\n",
    "  - unbound state: area where `lower_unbound` < $d$ < `upper_unbound`\n",
    "- finally you can calculate $\\Delta G_{PMF}$ by the following equation\n",
    "$$\n",
    "\\Delta G_{PMF} = - RT \\ln{\\frac{\\Sigma_b \\pi_i}{\\Sigma_u \\pi_i}}\n",
    "$$\n",
    "#### 2. Volume Correction Term $\\Delta G_v$\n",
    "- calculate the following volume correction term.\n",
    "$$\n",
    "\\Delta G_v = - RT \\ln{\\frac{V_u}{V_0}}\n",
    "$$\n",
    "- meanings of each term\n",
    "  - $V_u$ is the sampled volume of unbound state.\n",
    "    - can be approximated by the convex volume of in 3-D inter-COM distance of the unbond state .\n",
    "  - $V_0$ is the standard volume($=1661\\mathrm{\\AA^3}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vc_per_trial(\n",
    "    params: Parameters,\n",
    "    trial: int,\n",
    "    lower_unbound: float,\n",
    "    upper_unbound: float,\n",
    "):\n",
    "    # check the order of bound, unbound\n",
    "    if lower_unbound > upper_unbound:\n",
    "        params.logger.error(\"lower_unbound must be less than upper_unbound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if upper_unbound > params.cutoff:\n",
    "        params.logger.error(\"upper_unbound must be less than cut_off. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # check if csv file already exists\n",
    "    vc_csv = f\"{params.output_directory}/result_csvs/1d_VC.csv\"\n",
    "    if not Path(vc_csv).exists():\n",
    "        with open(vc_csv, \"w\") as f:\n",
    "            f.write(\"trial,lower_unbound,upper_unbound,volume,correction\\n\")\n",
    "    else:\n",
    "        df = pd.read_csv(vc_csv)\n",
    "        if len(\n",
    "                df[\n",
    "                    (df[\"trial\"]==trial)\n",
    "                    & (df[\"lower_unbound\"]==lower_unbound)\n",
    "                    & (df[\"upper_unbound\"]==upper_unbound)\n",
    "                ]\n",
    "            ) > 0:\n",
    "            # debug level to avoid too noisy output\n",
    "            params.logger.debug(f\"Volume correction for trial{trial:03} lower_unbound{lower_unbound} upper_unbound{upper_unbound} already calculated. Skipping\")\n",
    "            return\n",
    "    \n",
    "    params.logger.info(f\"Starting volume correction calculation for trial{trial:03}\")\n",
    "    # load\n",
    "    inter_COM_vec_all = []\n",
    "    feature_files_trial = list(\n",
    "        Path(f\"{params.feature_3d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "    )\n",
    "    feature_files_trial.sort()\n",
    "    for rep_path in feature_files_trial:\n",
    "        inter_COM_vec = np.load(rep_path)\n",
    "        max_distance = np.max(np.linalg.norm(inter_COM_vec, axis=1))\n",
    "        if max_distance > params.cutoff:\n",
    "            continue\n",
    "        inter_COM_vec_all.append(inter_COM_vec)\n",
    "    inter_COM_vec_all = np.concatenate(inter_COM_vec_all)\n",
    "\n",
    "    # select snapshots of unbound states\n",
    "    inter_COM_d_all = np.linalg.norm(inter_COM_vec_all, axis=1)\n",
    "    ix = np.where((inter_COM_d_all >= lower_unbound) & (inter_COM_d_all < upper_unbound))\n",
    "    inter_COM_vec = inter_COM_vec_all[ix]\n",
    "\n",
    "    # calculate volume correction\n",
    "    if len(inter_COM_vec) > 0:\n",
    "        hull = ConvexHull(inter_COM_vec)\n",
    "        volume = hull.volume * 1000  # [angstrome^3]\n",
    "        correction = params._RT * np.log(volume/1661) # [kcal/mol]\n",
    "    else:\n",
    "        volume = 0\n",
    "        correction = 0\n",
    "\n",
    "    # save to csv file\n",
    "    with open(vc_csv, \"a\") as f:\n",
    "        f.write(f\"{trial},{lower_unbound},{upper_unbound},{volume},{correction}\\n\")\n",
    "    \n",
    "    params.logger.info(f\"Finished volume correction calculation for trial{trial:03}\")\n",
    "\n",
    "\n",
    "def calc_binding_energy_1d(\n",
    "    params: Parameters,\n",
    "    n_clusters: int,\n",
    "    lag: int,\n",
    "    lower_bound: float,\n",
    "    upper_bound: float,\n",
    "    lower_unbound: float,\n",
    "    upper_unbound: float,\n",
    ") -> None:\n",
    "    # check the order of bound, unbound\n",
    "    if lower_bound >= upper_bound:\n",
    "        params.logger.error(\"lower_bound should be less than upper_bound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if lower_unbound >= upper_unbound:\n",
    "        params.logger.error(\"lower_unbound should be less than upper_unbound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if upper_bound > lower_unbound:\n",
    "        params.logger.error(\"upper_bound should be less than or equal to lower_unbound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if upper_unbound > params.cutoff:\n",
    "        params.logger.error(\"upper_unbound should be less than or equal to cutoff. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # check if csv file already exists\n",
    "    be_csv = f\"{params.output_directory}/result_csvs/1d_binding_energy_summary.csv\"\n",
    "    if not Path(be_csv).exists():\n",
    "        with open(be_csv, \"w\") as f:\n",
    "            f.write(\"trial,n_clusters,lag,lower_bound,upper_bound,lower_unbound,upper_unbound,dG_PMF,VC\\n\")\n",
    "\n",
    "    # calc volume correction\n",
    "    if params.do_volume_correction:\n",
    "        for trial in params.n_trial_for_calc:\n",
    "            calc_vc_per_trial(\n",
    "                params=params,\n",
    "                trial=trial,\n",
    "                lower_unbound=lower_unbound,\n",
    "                upper_unbound=upper_unbound,\n",
    "            )\n",
    "    \n",
    "    # calc binding energy\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        # check if already calculated\n",
    "        df = pd.read_csv(be_csv)\n",
    "        if len(\n",
    "                df[\n",
    "                    (df[\"trial\"]==trial)\n",
    "                    & (df[\"n_clusters\"]==n_clusters) \n",
    "                    & (df[\"lag\"]==lag) \n",
    "                    & (df[\"lower_bound\"]==lower_bound) \n",
    "                    & (df[\"upper_bound\"]==upper_bound) \n",
    "                    & (df[\"lower_unbound\"]==lower_unbound) \n",
    "                    & (df[\"upper_unbound\"]==upper_unbound)\n",
    "                ]\n",
    "            ) > 0:\n",
    "            params.logger.debug(\n",
    "                f\"Binding energy for trial{trial:03} \"\n",
    "                f\"n_clusters{n_clusters} \"\n",
    "                f\"lag{lag} \"\n",
    "                f\"lower_bound{lower_bound} \"\n",
    "                f\"upper_bound{upper_bound} \"\n",
    "                f\"lower_unbound{lower_unbound} \"\n",
    "                f\"upper_unbound{upper_unbound} \"\n",
    "                \"already exists. Skipped\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # load msm\n",
    "        msm_result_csv = f\"{params.output_directory}/result_csvs/1d_trial{trial:03}_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "        if not Path(msm_result_csv).exists():\n",
    "            params.logger.info(\n",
    "                f\"MSM result csv file not found. Skipping. First, run MSM.\"\n",
    "            )\n",
    "            continue\n",
    "        stat_dist_df = pd.read_csv(msm_result_csv)\n",
    "        cluster_center_d = stat_dist_df[\"cluster_center_d\"].values\n",
    "        stat_dists = stat_dist_df[\"cluster_center_pi\"].values\n",
    "\n",
    "        # select clusters where stationary distribution is not zero\n",
    "        mask = stat_dists > 0\n",
    "        cluster_center_d = cluster_center_d[mask]\n",
    "        stat_dists = stat_dists[mask]\n",
    "\n",
    "        # select clusters in bound/unbound states\n",
    "        ix_b = np.where((cluster_center_d >= lower_bound) & (cluster_center_d < upper_bound))\n",
    "        ix_u = np.where((cluster_center_d >= lower_unbound) & (cluster_center_d < upper_unbound))\n",
    "        pi_b = np.sum(stat_dists[ix_b])\n",
    "        pi_u = np.sum(stat_dists[ix_u])\n",
    "\n",
    "        # check if no cluster found in either bound/unbound state\n",
    "        if len(stat_dists[ix_b]) == 0 or len(stat_dists[ix_u]) == 0:\n",
    "            params.logger.info(\n",
    "                \"No cluster found in either bound/unbound state for \"\n",
    "                f\"trial{trial:03} \"\n",
    "                f\"n_clusters={n_clusters} \"\n",
    "                f\"lag={lag} \"\n",
    "                f\"lower_bound={lower_bound} \"\n",
    "                f\"upper_bound={upper_bound} \"\n",
    "                f\"lower_unbound={lower_unbound} \"\n",
    "                f\"upper_unbound={upper_unbound} . Skipping.\"\n",
    "                \"Please adjust the bound/unbound state definition\"\n",
    "            )\n",
    "            continue\n",
    "        dG = params._RT * np.log(pi_b / pi_u) # [kcal/mol]\n",
    "\n",
    "        # get vc information\n",
    "        if params.do_volume_correction:\n",
    "            vc_csv = f\"{params.output_directory}/result_csvs/1d_VC.csv\"\n",
    "            vc_df = pd.read_csv(vc_csv)\n",
    "            vc_df = vc_df[\n",
    "                (vc_df[\"lower_unbound\"]==lower_unbound) \n",
    "                & (vc_df[\"upper_unbound\"]==upper_unbound)\n",
    "            ]\n",
    "            vc = vc_df[vc_df[\"trial\"] == trial][\"correction\"].values[0]\n",
    "        else:\n",
    "            vc = 0\n",
    "        \n",
    "        # save to csv file\n",
    "        with open(be_csv, \"a\") as f:\n",
    "            f.write(f\"{trial},{n_clusters},{lag},{lower_bound},{upper_bound},{lower_unbound},{upper_unbound},{dG},{vc}\\n\")\n",
    "\n",
    "        # debug level to avoid too noisy output\n",
    "        params.logger.debug(\n",
    "            f\"Finished calculating binding energy for \"\n",
    "            f\"trial{trial:03} \"\n",
    "            f\"n_clusters={n_clusters} \"\n",
    "            f\"lag={lag} \"\n",
    "            f\"lower_bound={lower_bound} \"\n",
    "            f\"upper_bound={upper_bound} \"\n",
    "            f\"lower_unbound={lower_unbound} \"\n",
    "            f\"upper_unbound={upper_unbound} \"\n",
    "        )\n",
    "    params.logger.info(\n",
    "        f\"Finished calculating binding energy for all trials for \"\n",
    "        f\"n_clusters={n_clusters} \"\n",
    "        f\"lag={lag} \"\n",
    "        f\"lower_bound={lower_bound} \"\n",
    "        f\"upper_bound={upper_bound} \"\n",
    "        f\"lower_unbound={lower_unbound} \"\n",
    "        f\"upper_unbound={upper_unbound} \"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc binding energy\n",
    "\n",
    "for n_clusters in n_clusters_1d_main:\n",
    "    for lag in lags_1d_main:\n",
    "        calc_binding_energy_1d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag,\n",
    "            lower_bound=lower_bound,\n",
    "            upper_bound=upper_bound,\n",
    "            lower_unbound=lower_unbound,\n",
    "            upper_unbound=params.cutoff,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "be_csv = f\"{params.output_directory}/result_csvs/1d_binding_energy_summary.csv\"\n",
    "result_1d_df = pd.read_csv(be_csv)\n",
    "\n",
    "# calculate the total dG with volume correction\n",
    "result_1d_df[\"dG_total\"] = result_1d_df[\"dG_PMF\"] + result_1d_df[\"VC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "( \n",
    "    result_1d_df.query(\"n_clusters == '50' and lag == '40'\")\n",
    "    .groupby([\"n_clusters\", \"lag\", \"lower_bound\", \"upper_bound\", \"lower_unbound\", \"upper_unbound\"])\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_3d(\n",
    "        params: Parameters,\n",
    "        max_iter: int = 2000,\n",
    "):  \n",
    "    for n_clusters in params.n_clusters_for_try_3d:\n",
    "        # check\n",
    "        clustering_result_pkl = f\"{params.output_directory}/cluster_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if Path(clustering_result_pkl).exists():\n",
    "            params.logger.info(\n",
    "                f\"Clustering result obj for n_clusters={n_clusters} cutoff={params.cutoff} already exists. Skipping.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # cluster\n",
    "        params.logger.info(f\"Starting clustering for n_clusters={n_clusters} cutoff={params.cutoff}\")\n",
    "        # loads\n",
    "        features = []\n",
    "        for trial in params.n_trial_for_calc:\n",
    "            feature_files_trial = list(\n",
    "                Path(f\"{params.feature_3d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "            )\n",
    "            feature_files_trial.sort()\n",
    "            for rep_path in feature_files_trial:\n",
    "                features_per_rep = np.load(rep_path)\n",
    "                max_distance = np.max(np.linalg.norm(features_per_rep, axis=1))\n",
    "                if max_distance > params.cutoff:\n",
    "                    continue\n",
    "                else:\n",
    "                    features.append(features_per_rep)\n",
    "        \n",
    "        # clustering\n",
    "        estimator = deeptime.clustering.KMeans(\n",
    "            n_clusters=n_clusters, \n",
    "            max_iter=max_iter,\n",
    "            metric=\"euclidean\",\n",
    "            tolerance=1e-05,\n",
    "            init_strategy=\"kmeans++\",\n",
    "            fixed_seed=False,\n",
    "            n_jobs=None, \n",
    "            initial_centers=None,\n",
    "            progress=None\n",
    "        )\n",
    "        estimator.fit(np.concatenate(features))\n",
    "        clustering_trjs = []\n",
    "        for features_per_rep in features:\n",
    "            clustering_trjs.append(estimator.transform(features_per_rep))\n",
    "        clustering_model = estimator.fetch_model()\n",
    "\n",
    "        # save result \n",
    "        save_dict = {\n",
    "            \"trjs\": clustering_trjs,\n",
    "            \"model\": clustering_model\n",
    "        }\n",
    "        with open(clustering_result_pkl, \"wb\") as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "        \n",
    "        # create convergence figure\n",
    "        inertias = clustering_model.inertias\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot(inertias)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"inertia\")\n",
    "        plt.title(f\"n_clusters{n_clusters} clustering\")\n",
    "        plt.savefig(\n",
    "            f\"{params.output_directory}/images/clustering_converge_3d_n_clusters{n_clusters}_cut{params.cutoff}.png\", \n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            dpi=300\n",
    "        )\n",
    "        if params.show_picture:\n",
    "            plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        params.logger.info(f\"Finished clustering for n_clusters={n_clusters} cutoff={params.cutoff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "cluster_3d(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: Plot Histgram of $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_3d(\n",
    "    params: Parameters,\n",
    "    n_clusters: int, \n",
    ") -> None:\n",
    "    # check if clustering result exists\n",
    "    clustering_result_pkl = f\"{params.output_directory}/cluster_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "    if not Path(clustering_result_pkl).exists():\n",
    "        params.logger.info(\n",
    "            f\"Clustering obj for n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping. First, run clustering.\"\n",
    "        )\n",
    "        return\n",
    "    with open(clustering_result_pkl, \"rb\") as f:\n",
    "        save_dict = pickle.load(f)\n",
    "        clustering_model = save_dict[\"model\"]\n",
    "        center_d = np.linalg.norm(clustering_model.cluster_centers, axis=1)\n",
    "    \n",
    "    # load\n",
    "    features = []\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        feature_files_trial = list(\n",
    "            Path(f\"{params.feature_3d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "        )\n",
    "        feature_files_trial.sort()\n",
    "        for rep_path in feature_files_trial:\n",
    "            features_per_rep = np.load(rep_path)\n",
    "            max_distance = np.max(np.linalg.norm(features_per_rep, axis=1))\n",
    "            if max_distance > params.cutoff:\n",
    "                continue\n",
    "            else:\n",
    "                features.append(np.linalg.norm(features_per_rep, axis=1))\n",
    "    \n",
    "    # create figure\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.vlines(\n",
    "        center_d, \n",
    "        0, \n",
    "        10**10, \n",
    "        \"tab:red\", \n",
    "        linewidth=0.5,\n",
    "        label=\"cluster center\"\n",
    "    )\n",
    "    plt.vlines(\n",
    "        params.cutoff, \n",
    "        0, \n",
    "        10**10, \n",
    "        \"magenta\", \n",
    "        linewidth=0.5,\n",
    "        label=\"cutoff\"\n",
    "    )\n",
    "    plt.hist(\n",
    "        np.concatenate(features), \n",
    "        bins=100, \n",
    "        alpha=0.5\n",
    "    )\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"$d$ [nm]\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Inter-COM distance distribution\")\n",
    "    # put legend on right hand\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.ylim(1, 10**6)\n",
    "    plt.savefig(\n",
    "        f\"{params.output_directory}/images/hist_3d_n_clusters{n_clusters}_cut{params.cutoff}.png\", \n",
    "        bbox_inches='tight',\n",
    "        pad_inches=0.1,   \n",
    "        dpi=300\n",
    "    )\n",
    "    if params.show_picture:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    params.logger.info(f\"Finished histogram plotting for n_clusters={n_clusters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "for n_clusters in params.n_clusters_for_try_3d:\n",
    "    plot_hist_3d(\n",
    "        params=params,\n",
    "        n_clusters=n_clusters\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: Plot inertias\n",
    "- plot inertias along n_clusters\n",
    "- can be used to decide appropriate n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inertia_3d(params: Parameters):\n",
    "    n_clusters_list = []\n",
    "    inertias = []\n",
    "    for n_clusters in params.n_clusters_for_try_3d:\n",
    "        clustering_result_pkl = f\"{params.output_directory}/cluster_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if not Path(clustering_result_pkl).exists():\n",
    "            params.logger.info(\n",
    "                f\"Clustering result obj for n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping.\"\n",
    "            )\n",
    "            continue\n",
    "        with open(clustering_result_pkl, \"rb\") as f:\n",
    "            save_dict = pickle.load(f)\n",
    "            clustering_model = save_dict[\"model\"]\n",
    "        n_clusters_list.append(n_clusters)\n",
    "        inertias.append(clustering_model.inertia)\n",
    "    \n",
    "    # check \n",
    "    if len(n_clusters_list) == 0:\n",
    "        params.logger.info(f\"No clustering result found for 3d. No need to plot. Skipping.\")\n",
    "        return\n",
    "    elif len(n_clusters_list) == 1:\n",
    "        params.logger.info(f\"Only one clustering result found. Cannot plot. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # plot\n",
    "    params.logger.info(f\"Starting plotting inertia for 3d\")\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.plot(n_clusters_list, inertias, marker=\".\")\n",
    "    plt.xlabel(\"number of clusters\")\n",
    "    plt.ylabel(\"inertia\")\n",
    "    plt.title(f\"3d\")\n",
    "    plt.savefig(\n",
    "        f\"{params.output_directory}/images/inertia_3d_cut{params.cutoff}.png\", \n",
    "        bbox_inches='tight',\n",
    "        pad_inches=0.1,\n",
    "        dpi=300\n",
    "    )\n",
    "    if params.show_picture:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    params.logger.info(f\"Finished plotting inertia for 3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "plot_inertia_3d(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: Build MSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_msm_3d(\n",
    "    params: Parameters,\n",
    ") -> None:\n",
    "    for n_clusters in params.n_clusters_for_try_3d:\n",
    "        # load or initialize msm result\n",
    "        msm_result_pkl = f\"{params.output_directory}/MSM_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if not Path(msm_result_pkl).exists():\n",
    "            msm_result = dict()\n",
    "        else:\n",
    "            with open(msm_result_pkl, \"rb\") as f:\n",
    "                msm_result = pickle.load(f)\n",
    "        \n",
    "        # load or itinitialize count model result\n",
    "        count_result_pkl = f\"{params.output_directory}/Count_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if not Path(count_result_pkl).exists():\n",
    "            count_result = dict()\n",
    "        else:\n",
    "            with open(count_result_pkl, \"rb\") as f:\n",
    "                count_result = pickle.load(f)\n",
    "        \n",
    "        # check if all clustering results exist\n",
    "        clustering_result_pkl = f\"{params.output_directory}/cluster_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if not Path(clustering_result_pkl).exists():\n",
    "            params.logger.info(\n",
    "                f\"Clustering obj for n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping. First, run clustering.\"\n",
    "            )\n",
    "            continue\n",
    "        with open(clustering_result_pkl, \"rb\") as f:\n",
    "            save_dict = pickle.load(f)\n",
    "            trjs = save_dict[\"trjs\"]\n",
    "            clustering_model = save_dict[\"model\"]\n",
    "        \n",
    "        # build msm for all lags in params.lags_for_try_3d\n",
    "        for lag in params.lags_for_try_3d:\n",
    "            # skip if already exits\n",
    "            if lag in msm_result.keys():\n",
    "                params.logger.info(\n",
    "                    f\"MSM result for n_clusters={n_clusters} lag={lag} already exists. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            params.logger.info(f\"Starting MSM for n_clusters={n_clusters} lag={lag}\")\n",
    "            # build count matrix\n",
    "            count_estimator = deeptime.markov.TransitionCountEstimator(\n",
    "                lagtime=lag, \n",
    "                count_mode=\"effective\",\n",
    "                n_states=None,\n",
    "                sparse=False\n",
    "            )\n",
    "            count_model = count_estimator.fit(\n",
    "                trjs, \n",
    "                # \"n_jobs = None\" means using all cores. \n",
    "                # If you want to save the memory at the cost of speed, \n",
    "                # specify n_jobs = integer meaning maximum number ofcores to use.\n",
    "                n_jobs=None, \n",
    "            ).fetch_model()\n",
    "\n",
    "            # build msm\n",
    "            msm_estimator = deeptime.markov.msm.BayesianMSM(\n",
    "                n_samples=100,\n",
    "                n_steps=None, # 1 step = 1 interval in the trajectory\n",
    "                reversible=True,\n",
    "                stationary_distribution_constraint=None,\n",
    "                sparse=False,\n",
    "                maxiter=1000000,\n",
    "                maxerr=1e-08,\n",
    "                lagtime=None,\n",
    "            )\n",
    "            try:\n",
    "                msm_model = msm_estimator.fit(count_model).fetch_model()\n",
    "            except Exception as e:\n",
    "                params.logger.error(f\"Error occurred in building MSM at n_clusters={n_clusters} lag={lag}\")\n",
    "                params.logger.error(f\"error message: {e}\")\n",
    "                continue\n",
    "\n",
    "            # save msm result\n",
    "            msm_result[lag] = msm_model\n",
    "            count_result[lag] = count_model\n",
    "            with open(msm_result_pkl, \"wb\") as f:\n",
    "                pickle.dump(msm_result, f)\n",
    "            with open(count_result_pkl, \"wb\") as f:\n",
    "                pickle.dump(count_result, f)\n",
    "                \n",
    "            # save cluster center coordinates and corresponding stationary destribution\n",
    "            n_clusters_in_clustering = len(clustering_model.cluster_centers)\n",
    "            n_clusters_in_msm = msm_model.prior.n_states\n",
    "            cluster_centers = clustering_model.cluster_centers\n",
    "\n",
    "            stationary_distribution = np.zeros((len(msm_model.samples), n_clusters_in_clustering))\n",
    "            largest_connected_set = deeptime.markov.tools.estimation.largest_connected_set(\n",
    "                count_model.count_matrix,\n",
    "                directed=True\n",
    "            )\n",
    "            for i, sample in enumerate(msm_model.samples):\n",
    "                stationary_distribution[i, largest_connected_set] = sample.stationary_distribution\n",
    "            if n_clusters_in_clustering != n_clusters_in_msm:\n",
    "                params.logger.info(\n",
    "                    f\"The number of clusters for clustering and msm are different for n_clusters={n_clusters} lag={lag}\"\n",
    "                )\n",
    "                params.logger.info(f\"Number of clusters for clustering: {n_clusters_in_clustering}\")\n",
    "                params.logger.info(f\"Number of clusters in the largest connected set: {n_clusters_in_msm}\")\n",
    "            center_pi_csv = f\"{params.output_directory}/result_csvs/3d_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "            with open(center_pi_csv, \"w\") as f:\n",
    "                header=\"cluster_center_x,cluster_center_y,cluster_center_z,\"\n",
    "                for i in range(len(msm_model.samples)):\n",
    "                    header += f\"cluster_center_pi_{i+1},\"\n",
    "                header = header[:-1]\n",
    "                header += \"\\n\"\n",
    "                f.write(header)\n",
    "                num_clusters = len(cluster_centers)\n",
    "                for cluster_ind in range(num_clusters):\n",
    "                    center_x = cluster_centers[cluster_ind][0]\n",
    "                    center_y = cluster_centers[cluster_ind][1]\n",
    "                    center_z = cluster_centers[cluster_ind][2]\n",
    "                    pis_in_samples = stationary_distribution[:,cluster_ind]\n",
    "                    pis_str = \",\".join([str(pi) for pi in pis_in_samples])[:-1]\n",
    "                    f.write(f\"{center_x},{center_y},{center_z},{pis_str}\\n\")\n",
    "            params.logger.info(f\"Finished MSM for n_clusters={n_clusters} lag={lag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build msm\n",
    "build_msm_3d(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: Plot ITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_its_3d(\n",
    "    params: Parameters,\n",
    ") -> None:\n",
    "    for n_clusters in params.n_clusters_for_try_3d:\n",
    "        # load or initialize msm result\n",
    "        msm_result_pkl = f\"{params.output_directory}/MSM_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "        if not Path(msm_result_pkl).exists():\n",
    "            params.logger.info(\n",
    "                f\"MSM result obj for n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping.\"\n",
    "                f\"First, run MSM.\"\n",
    "            )\n",
    "            continue\n",
    "        else:\n",
    "            with open(msm_result_pkl, \"rb\") as f:\n",
    "                msm_result = pickle.load(f)\n",
    "        \n",
    "        # accumulate msm models and calculate implied timescales\n",
    "        params.logger.info(f\"Starting plotting ITS for n_clusters={n_clusters}\")\n",
    "        msm_models = []\n",
    "        for lag in sorted(msm_result.keys()):\n",
    "            msm_models.append(msm_result[lag])\n",
    "        its_data = deeptime.util.validation.implied_timescales(\n",
    "            models=msm_models,\n",
    "            n_its=10, # decrease this number for n_clusters < 11\n",
    "        )\n",
    "\n",
    "        # plot its\n",
    "        plt.figure(figsize=(3, 2))\n",
    "        deeptime.plots.plot_implied_timescales(\n",
    "            its_data,\n",
    "            n_its=10, # decrease this number for n_clusters < 11\n",
    "            process=None,\n",
    "            show_mle=True,\n",
    "            show_sample_mean=True, # ignored\n",
    "            show_sample_confidence=True, # ignored\n",
    "            show_cutoff=True, # ignored\n",
    "            sample_confidence=0.95, # ignored\n",
    "            colors=None,\n",
    "            # ax=None,\n",
    "        )\n",
    "        plt.title(f\"n_clusters{n_clusters}\")\n",
    "        plt.xlabel(\"lag time (steps)\") # 1 step = 1 interval in the trajectory\n",
    "        plt.ylabel(\"implied timescale (steps)\") # 1 step = 1 interval in the trajectory\n",
    "        plt.yscale(\"log\")\n",
    "        plt.savefig(\n",
    "            f\"{params.output_directory}/images/its_3d_n_clusters{n_clusters}_cut{params.cutoff}.png\", \n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            dpi=300\n",
    "        )\n",
    "        if params.show_picture:\n",
    "            plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        params.logger.info(f\"Finished plotting ITS for n_clusters={n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot its\n",
    "plot_its_3d(params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select n_clusters and lag time to use for later analysis in this notebook.\n",
    "\n",
    "You can select parameters where implied timescales above is converged.\n",
    "\n",
    "The parameters selected here will be subsequently used for plotting FEL, calculating binding free energy in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the default value\n",
    "n_clusters_3d_main = params.n_clusters_for_try_3d\n",
    "lags_3d_main = params.lags_for_try_3d\n",
    "\n",
    "# It is recommended to change the values as shown below (example) to save time\n",
    "# n_clusters_3d_main = [500, 1000]\n",
    "# lags_3d_main = [30, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: FEL along $d$\n",
    "\n",
    "- Objective:\n",
    "  Calculate the free energy landscape (FEL) along $d$ using all trials collectively.\n",
    "\n",
    "- Approach:  \n",
    "  In contrast to the 1D analysis where each trial is processed separately, the 3D analysis is performed on the combined dataset of all trials.  \n",
    "  Bayesian MSM generates a large number of samples, which allows us to estimate error bounds.  \n",
    "  Only the mean FEL with its error bounds (mean $\\pm$ error) is plotted, rather than all individual samples.\n",
    "\n",
    "1. Determine the Bin Ranges:\n",
    "\n",
    "   - Let the cluster center of the $l$th cluster be denoted as $d^{l}$.\n",
    "   - Compute the overall minimum and maximum among all $d^{l}$ obtained from the clustering results.\n",
    "   - Divide the range between these minimum and maximum values into `params.n_bins` equal intervals.\n",
    "   - Define:\n",
    "     - The center of each interval as $d_i$.\n",
    "     - The length of each interval as $\\delta d$.\n",
    "\n",
    "2. Compute the FEL Using Bayesian MSM:\n",
    "\n",
    "   - Utilize the large number of samples generated by Bayesian MSM to estimate the FEL at each bin center $d_i$.\n",
    "   - For each $d_i$, compute the free energy as:\n",
    "     $$\n",
    "     \\Delta W(d_i) = - RT \\ln \\left( \\sum_{d_i - \\frac{\\delta d}{2} \\le d^l \\le d_i + \\frac{\\delta d}{2}} \\pi_l \\right)\n",
    "     $$\n",
    "     where $\\pi_l$ is the stationary probability associated with the $l$ th cluster.\n",
    "   - From the ensemble of Bayesian MSM samples, derive the standard error of $\\Delta W(d_i)$.\n",
    "\n",
    "3. Plot the Averaged FEL with Error Bounds:\n",
    "\n",
    "   - Plot only the mean FEL with its error bounds, i.e., $\\Delta W(d_i) \\pm \\text{error}$.\n",
    "   - Do not plot individual samples; only the aggregated statistics (mean $\\pm$ error) are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fel_along_d_3d(\n",
    "    params:Parameters,\n",
    "    n_clusters: int,\n",
    "    lag: int,\n",
    ") -> None:\n",
    "    # log\n",
    "    params.logger.info(\n",
    "        f\"Starting plotting FEL along d for n_clusters={n_clusters} lag={lag}\"\n",
    "    )\n",
    "\n",
    "    # load\n",
    "    msm_result_csv = f\"{params.output_directory}/result_csvs/3d_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "    df = pd.read_csv(msm_result_csv)\n",
    "    cluster_center_x = df[\"cluster_center_x\"].values\n",
    "    cluster_center_y = df[\"cluster_center_y\"].values\n",
    "    cluster_center_z = df[\"cluster_center_z\"].values\n",
    "    cluster_center_d = np.sqrt(cluster_center_x**2 + cluster_center_y**2 + cluster_center_z**2)\n",
    "    stat_dists = df.iloc[:, 3:].values # shape=(n_clusters, n_samples)\n",
    "\n",
    "    # binning\n",
    "    bins = np.linspace(\n",
    "        start=min(cluster_center_d), \n",
    "        stop=max(cluster_center_d), \n",
    "        num=params.nbins\n",
    "    )\n",
    "    bins -= (bins[1] - bins[0]) / 2\n",
    "    bin_centers = bins[:-1] + (bins[1] - bins[0]) / 2\n",
    "    bin_energy_means = np.zeros(len(bins) - 1)\n",
    "    bin_energy_stds = np.zeros(len(bins) - 1)\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_mask = (cluster_center_d >= bins[i]) & (cluster_center_d < bins[i+1])\n",
    "        pi_sum = np.sum(stat_dists[bin_mask,:], axis=0)\n",
    "        energies = params._RT * np.log(pi_sum)\n",
    "        bin_energy_means[i] = np.mean(energies)\n",
    "        bin_energy_stds[i] = np.std(energies)\n",
    "    bin_stat_dist_stes = bin_energy_stds / np.sqrt(len(stat_dists))\n",
    "    bin_energy_means -= np.min(bin_energy_means)\n",
    "\n",
    "    # plot\n",
    "    plt.plot(\n",
    "        bin_centers, \n",
    "        bin_energy_means, \n",
    "        color=\"tab:blue\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.errorbar(\n",
    "        bin_centers, \n",
    "        bin_energy_means, \n",
    "        yerr=bin_stat_dist_stes, \n",
    "        fmt=\"o\", \n",
    "        label=r\"Mean $\\pm$SE\",\n",
    "        color=\"tab:blue\",\n",
    "        capsize=3,\n",
    "    )\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 20)\n",
    "    plt.title(f\"n_clusters={n_clusters} lag={lag}\")\n",
    "    plt.xlabel(r\"$d$ [nm]\")\n",
    "    plt.ylabel(r\"$\\Delta W$ [kcal/mol]\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\n",
    "        f\"{params.output_directory}/images/fel_3d_n_clusters{n_clusters}_lag{lag}.png\",\n",
    "        bbox_inches='tight',\n",
    "        pad_inches=0.1,\n",
    "        dpi=300,\n",
    "    )\n",
    "    if params.show_picture:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    params.logger.info(\n",
    "        f\"Finished plotting FEL along d for n_clusters={n_clusters} lag={lag}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in n_clusters_3d_main:\n",
    "    for lag in lags_3d_main:\n",
    "        plot_fel_along_d_3d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the bound/unbound state according to 1d-fel of your system plotted above.\n",
    "\n",
    "If you don't set the definition, you can't calculate the binding free energy.\n",
    "- the meaning of each variable is written in `1D: Binding Free Energy Calculation` section\n",
    "- unit is [nm]\n",
    "- set as lower_bound < lower_unbound <= upper_unbound < upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values are invalid to prevent mistakes. Please modify them to fit your system.\n",
    "\n",
    "# definition for bound-state\n",
    "lower_bound = 0 # 0 is recommended\n",
    "upper_bound = 5\n",
    "\n",
    "# definition for unbound-state\n",
    "lower_unbound = 4\n",
    "upper_unbound = params.cutoff # params.cutoff is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: Binding Free Energy\n",
    "- Calculate the binding free enerygy in the same way as `1D: Binding Free Energy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vc_all_trials(\n",
    "    params: Parameters,\n",
    "    lower_unbound: float,\n",
    "    upper_unbound: float,\n",
    ") -> None:\n",
    "    # check order of lower_unbound, upper_unbound\n",
    "    if lower_unbound > upper_unbound:\n",
    "        params.logger.error(\"lower_unbound must be less than upper_unbound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if upper_unbound > params.cutoff:\n",
    "        params.logger.error(\"upper_unbound must be less than cut_off. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # check if vc csv file exists\n",
    "    vc_csv = f\"{params.output_directory}/result_csvs/3d_VC.csv\"\n",
    "    if not Path(vc_csv).exists():\n",
    "        with open(vc_csv, \"w\") as f:\n",
    "            f.write(\"lower_unbound,upper_unbound,volume,correction\\n\")\n",
    "    else:\n",
    "        # check if already calculated\n",
    "        df = pd.read_csv(vc_csv)\n",
    "        if len(df[(df[\"lower_unbound\"]==lower_unbound) & (df[\"upper_unbound\"]==upper_unbound)]) > 0:\n",
    "            params.logger.debug(\n",
    "                f\"volume correction for lower_unbound={lower_unbound} upper_unbound={upper_unbound} already exists. Skipped\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "    params.logger.info(\n",
    "        f\"Starting volume correction calculation for lower_unbound={lower_unbound} upper_unbound={upper_unbound}\"\n",
    "    )\n",
    "    # calc vc\n",
    "    inter_COM_vec_all = []\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        feature_files_trial = list(\n",
    "            Path(f\"{params.feature_3d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "        )\n",
    "        feature_files_trial.sort()\n",
    "        for rep_path in feature_files_trial:\n",
    "            inter_COM_vec = np.load(rep_path)\n",
    "            max_distance = np.max(np.linalg.norm(inter_COM_vec, axis=1))\n",
    "            if max_distance > params.cutoff:\n",
    "                continue\n",
    "            inter_COM_vec_all.append(inter_COM_vec)\n",
    "    inter_COM_vec_all = np.concatenate(inter_COM_vec_all)\n",
    "\n",
    "    # select snapshots of unbound states\n",
    "    inter_COM_d_all = np.linalg.norm(inter_COM_vec_all, axis=1)\n",
    "    ix = np.where((inter_COM_d_all >= lower_unbound) & (inter_COM_d_all < upper_unbound))\n",
    "    inter_COM_vec = inter_COM_vec_all[ix]\n",
    "\n",
    "    # calculate volume correction\n",
    "    if len(inter_COM_vec) > 0:\n",
    "        hull = ConvexHull(inter_COM_vec)\n",
    "        volume = hull.volume * 1000  # [angstrome^3]\n",
    "        correction = params._RT * np.log(volume/1661) # [kcal/mol]\n",
    "    else:\n",
    "        volume = 0\n",
    "        correction = 0\n",
    "\n",
    "    # save to csv file\n",
    "    with open(vc_csv, \"a\") as f:\n",
    "        f.write(f\"{lower_unbound},{upper_unbound},{volume},{correction}\\n\")\n",
    "\n",
    "    params.logger.info(\n",
    "        f\"Finished volume correction calculation for lower_unbound={lower_unbound} upper_unbound={upper_unbound}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_binding_energy_3d(\n",
    "    params: Parameters,\n",
    "    n_clusters: int,\n",
    "    lag: int,\n",
    "    lower_bound: float,\n",
    "    upper_bound: float,\n",
    "    lower_unbound: float,\n",
    "    upper_unbound: float,\n",
    ") -> None:\n",
    "    # check the order of bound, unbound\n",
    "    if lower_bound >= upper_bound:\n",
    "        params.logger.error(\"lower_bound should be less than upper_bound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if lower_unbound >= upper_unbound:\n",
    "        params.logger.error(\"lower_unbound should be less than upper_unbound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if upper_bound > lower_unbound:\n",
    "        params.logger.error(\"upper_bound should be less than or equal to lower_unbound. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    if upper_unbound > params.cutoff:\n",
    "        params.logger.error(\"upper_unbound should be less than or equal to cutoff. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # check if csv file exists\n",
    "    be_csv = f\"{params.output_directory}/result_csvs/3d_binding_energy_summary.csv\"\n",
    "    if not Path(be_csv).exists():\n",
    "        with open(be_csv, \"w\") as f:\n",
    "            f.write(\"n_clusters,lag,lower_bound,upper_bound,lower_unbound,upper_unbound,dG_PMF,dG_ste,VC\\n\")\n",
    "    else:\n",
    "        # check if already calculated\n",
    "        df = pd.read_csv(be_csv)\n",
    "        if len(\n",
    "            df[\n",
    "                (df[\"n_clusters\"] == n_clusters)\n",
    "                & (df[\"lag\"] == lag)\n",
    "                & (df[\"lower_bound\"] == lower_bound)\n",
    "                & (df[\"upper_bound\"] == upper_bound)\n",
    "                & (df[\"lower_unbound\"] == lower_unbound)\n",
    "                & (df[\"upper_unbound\"] == upper_unbound)\n",
    "            ]\n",
    "        ) > 0:\n",
    "            params.logger.debug(\n",
    "                f\"Binding energy for n_clusters={n_clusters} \"\n",
    "                f\"lag={lag} \"\n",
    "                f\"lower_bound={lower_bound} \"\n",
    "                f\"upper_bound={upper_bound} \"\n",
    "                f\"lower_unbound={lower_unbound} \"\n",
    "                f\"upper_unbound={upper_unbound} \"\n",
    "                \"already calculated. Skipping\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "    # calc vc\n",
    "    if params.do_volume_correction:\n",
    "        vc_csv = f\"{params.output_directory}/result_csvs/3d_VC.csv\"\n",
    "        calc_vc_all_trials(\n",
    "            params=params,\n",
    "            lower_unbound=lower_unbound,\n",
    "            upper_unbound=upper_unbound,\n",
    "        )\n",
    "    \n",
    "    # calc binding energy\n",
    "    # load\n",
    "    msm_result_csv = f\"{params.output_directory}/result_csvs/3d_n_clusters{n_clusters}_lag{lag}_cut{params.cutoff}.csv\"\n",
    "    if not Path(msm_result_csv).exists():\n",
    "        params.logger.info(\n",
    "            f\"MSM result csv file for n_clusters={n_clusters} lag={lag} doesn't exist. Skipping.\"\n",
    "            f\"First, run MSM.\"\n",
    "        )\n",
    "        return\n",
    "    params.logger.info(\n",
    "        f\"Starting calculating binding energy for n_clusters={n_clusters} lag={lag} \"\n",
    "        f\"lower_bound={lower_bound} upper_bound={upper_bound} lower_unbound={lower_unbound} upper_unbound={upper_unbound}\"\n",
    "    )\n",
    "    df = pd.read_csv(msm_result_csv)\n",
    "    cluster_center_x = df[\"cluster_center_x\"].values\n",
    "    cluster_center_y = df[\"cluster_center_y\"].values\n",
    "    cluster_center_z = df[\"cluster_center_z\"].values\n",
    "    cluster_center_d = np.sqrt(cluster_center_x**2 + cluster_center_y**2 + cluster_center_z**2)\n",
    "    stat_dists = df.iloc[:, 3:].values # shape=(n_clusters, n_samples)\n",
    "\n",
    "    # select clusters in bound/unbound states\n",
    "    ix_b = np.where((cluster_center_d >= lower_bound) & (cluster_center_d < upper_bound))[0]\n",
    "    ix_u = np.where((cluster_center_d >= lower_unbound) & (cluster_center_d < upper_unbound))[0]\n",
    "    pi_b = np.sum(stat_dists[ix_b,:], axis=0)\n",
    "    pi_u = np.sum(stat_dists[ix_u,:], axis=0)\n",
    "    dG = params._RT * np.log(pi_b / pi_u) # [kcal/mol]\n",
    "    dG_mean = np.mean(dG)\n",
    "    dG_std = np.std(dG)\n",
    "    dG_ste = dG_std / len(dG) ** 0.5\n",
    "\n",
    "    # get vc information\n",
    "    if params.do_volume_correction:\n",
    "        vc_csv = f\"{params.output_directory}/result_csvs/3d_VC.csv\"\n",
    "        vc_df = pd.read_csv(vc_csv)\n",
    "        vc_df = vc_df[(vc_df[\"lower_unbound\"]==lower_unbound) & (vc_df[\"upper_unbound\"]==upper_unbound)]\n",
    "        vc = vc_df[\"correction\"].values[0]\n",
    "    else:\n",
    "        vc = 0\n",
    "    \n",
    "    # save to csv file\n",
    "    with open(be_csv, \"a\") as f:\n",
    "        f.write(f\"{n_clusters},{lag},{lower_bound},{upper_bound},{lower_unbound},{upper_unbound},{dG_mean},{dG_ste},{vc}\\n\")\n",
    "\n",
    "    params.logger.info(\n",
    "        f\"Finished calculating binding energy for n_clusters={n_clusters} lag={lag} \"\n",
    "        f\"lower_bound={lower_bound} upper_bound={upper_bound} lower_unbound={lower_unbound} upper_unbound={upper_unbound}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in n_clusters_3d_main:\n",
    "    for lag in lags_3d_main:\n",
    "        calc_binding_energy_3d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag,\n",
    "            lower_bound=lower_bound,\n",
    "            upper_bound=upper_bound,\n",
    "            lower_unbound=lower_unbound,\n",
    "            upper_unbound=params.cutoff,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "be_csv = f\"{params.output_directory}/result_csvs/3d_binding_energy_summary.csv\"\n",
    "result_3d_df = pd.read_csv(be_csv)\n",
    "\n",
    "# calculate the total dG with volume correction\n",
    "result_3d_df[\"dG_total\"] = result_3d_df[\"dG_PMF\"] + result_3d_df[\"VC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "( \n",
    "    result_3d_df.query(\"n_clusters == '100' and lag == '40'\")\n",
    "    .groupby([\"n_clusters\", \"lag\", \"lower_bound\", \"upper_bound\", \"lower_unbound\", \"upper_unbound\"])\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: $k_{on}, k_{off}$\n",
    "- calculate the rate constant $k_{on}, k_{off}$ using MFPTe\n",
    "- The relationship between MFPT and $k_{on}, k_{off}$ is as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "k_{off} &= \\frac{1}{MFPT_{off}} \\\\\n",
    "k_{on} &= \\frac{1}{MFPT_{on} C_{ligand}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- where $MFPT_{off}$ is the mean first passage time(MFPT) from the bound state to the unbound state, $MFPT_{on}$ is MFPT from the unbound state to the bound state, and $C_{ligand}$ is the ligand concentration.\n",
    "\n",
    "- MFPT can be calculated from MSM theory. See [deeptime web site](https://deeptime-ml.github.io/latest/api/generated/deeptime.markov.tools.analysis.mfpt.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rate_constant_3d(\n",
    "    params: Parameters, \n",
    "    n_clusters: int, \n",
    "    lag: int,\n",
    "    lower_bound: float,\n",
    "    upper_bound: float,\n",
    "    lower_unbound: float,\n",
    "    upper_unbound: float,\n",
    ") -> None:\n",
    "    # check if it already calculated\n",
    "    rc_csv = f\"{params.output_directory}/result_csvs/3d_rate_constant_summary.csv\"\n",
    "    if not Path(rc_csv).exists():\n",
    "        with open(rc_csv, \"w\") as f:\n",
    "            f.write(\"n_clusters,lag,lower_bound,upper_bound,lower_unbound,upper_unbound,koff_mean,koff_ste,kon_mean,kon_ste\\n\")\n",
    "    else:\n",
    "        # check if already calculated\n",
    "        df = pd.read_csv(rc_csv)\n",
    "        if len(\n",
    "            df[\n",
    "                (df[\"n_clusters\"] == n_clusters)\n",
    "                & (df[\"lag\"] == lag)\n",
    "                & (df[\"lower_bound\"] == lower_bound)\n",
    "                & (df[\"upper_bound\"] == upper_bound)\n",
    "                & (df[\"lower_unbound\"] == lower_unbound)\n",
    "                & (df[\"upper_unbound\"] == upper_unbound)\n",
    "            ]\n",
    "        ) > 0:\n",
    "            params.logger.info(\n",
    "                f\"Rate constant for n_clusters={n_clusters} lag={lag} \"\n",
    "                f\"lower_bound={lower_bound} upper_bound={upper_bound} \"\n",
    "                f\"lower_unbound={lower_unbound} upper_unbound={upper_unbound} \"\n",
    "                f\"already calculated. Skipping.\"\n",
    "            )\n",
    "            return\n",
    "    \n",
    "    # check the order of bound, unbound\n",
    "    with open(f\"{params.output_directory}/cluster_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\", \"rb\") as f:\n",
    "        cluster_pkl = pickle.load(f)\n",
    "    with open(f\"{params.output_directory}/MSM_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\", \"rb\") as f:\n",
    "        msm_pkl = pickle.load(f)\n",
    "    with open(f\"{params.output_directory}/Count_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\", \"rb\") as f:\n",
    "        count_pkl = pickle.load(f)\n",
    "\n",
    "    cluster_centers = cluster_pkl[\"model\"].cluster_centers\n",
    "    largest_connected_set = deeptime.markov.tools.estimation.largest_connected_set(\n",
    "        count_pkl[lag].count_matrix,\n",
    "        directed=True\n",
    "    )\n",
    "    center_d = np.linalg.norm(cluster_centers, axis=1)[largest_connected_set]\n",
    "    ix_u = np.where((center_d > lower_unbound) & (center_d < upper_unbound))[0]\n",
    "    ix_b = np.where((center_d > lower_bound) & (center_d < upper_bound))[0]\n",
    "    \n",
    "    mfpt_on_list = []\n",
    "    mfpt_off_list = []\n",
    "    for i in range(len(msm_pkl[lag].samples)):\n",
    "        T = msm_pkl[lag].samples[i].transition_matrix\n",
    "        sd = msm_pkl[lag].samples[i].stationary_distribution\n",
    "        mfpt_off = deeptime.markov.tools.analysis.mfpt(\n",
    "            T = T,\n",
    "            target=ix_u,\n",
    "            origin=ix_b,\n",
    "            tau=lag,\n",
    "            mu=sd\n",
    "        )\n",
    "        mfpt_on = deeptime.markov.tools.analysis.mfpt(\n",
    "            T = T,\n",
    "            target=ix_b,\n",
    "            origin=ix_u,\n",
    "            tau=lag,\n",
    "            mu=sd\n",
    "        )\n",
    "        mfpt_on_list.append(mfpt_on)\n",
    "        mfpt_off_list.append(mfpt_off)\n",
    "\n",
    "    mfpt_off_arr = np.array(mfpt_off_list)\n",
    "    mfpt_on_arr = np.array(mfpt_on_list)\n",
    "\n",
    "    koff_arr = 1 / mfpt_off_arr\n",
    "    kon_arr = 1 / mfpt_on_arr / params.ligand_concentration\n",
    "\n",
    "    koff_mean = np.mean(koff_arr) # step^-1\n",
    "    kon_mean = np.mean(kon_arr) # step^-1 M^-1\n",
    "\n",
    "    koff_ste = np.std(koff_arr) / len(koff_arr)**0.5\n",
    "    kon_ste = np.std(kon_arr) / len(kon_arr)**0.5\n",
    "\n",
    "    scaling_factor = 10**12 / params.dt # [step] to [second]\n",
    "    koff_mean *= scaling_factor # s^-1\n",
    "    kon_mean *= scaling_factor # s^-1 M^-1\n",
    "    koff_ste *= scaling_factor\n",
    "    kon_ste *= scaling_factor\n",
    "\n",
    "    # save\n",
    "    with open(rc_csv, \"a\") as f:\n",
    "        f.write(f\"{n_clusters},{lag},{lower_bound},{upper_bound},{lower_unbound},{upper_unbound},{koff_mean},{koff_ste},{kon_mean},{kon_ste}\\n\")\n",
    "\n",
    "    # log\n",
    "    params.logger.info(\n",
    "        f\"Finished calculating rate constants for n_clusters={n_clusters} lag={lag} \"\n",
    "        f\"lower_bound={lower_bound} upper_bound={upper_bound} \"\n",
    "        f\"lower_unbound={lower_unbound} upper_unbound={upper_unbound}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in n_clusters_3d_main:\n",
    "    for lag in lags_3d_main:\n",
    "        calc_rate_constant_3d(\n",
    "            params=params,\n",
    "            n_clusters=n_clusters,\n",
    "            lag=lag,\n",
    "            lower_bound=lower_bound,\n",
    "            upper_bound=upper_bound,\n",
    "            lower_unbound=lower_unbound,\n",
    "            upper_unbound=params.cutoff,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "rc_csv = f\"{params.output_directory}/result_csvs/3d_rate_constant_summary.csv\"\n",
    "result_rc_df = pd.read_csv(rc_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "( \n",
    "    result_rc_df.query(\"n_clusters == '100' and lag == '40'\")\n",
    "    .groupby([\"n_clusters\", \"lag\", \"lower_bound\", \"upper_bound\", \"lower_unbound\", \"upper_unbound\"])\n",
    "    .mean()\n",
    ")\n",
    "# koff [s^-1]\n",
    "# kon [s^-1 M^-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D: FEL on 2D plane\n",
    "- plot the FEL on the x-y, x-z, and y-z planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fel_each_2d(\n",
    "    params: Parameters,\n",
    "    n_clusters: int,\n",
    "    lag: int,\n",
    "    coord_names: list[int],\n",
    ") -> None:\n",
    "    # load raw feature data\n",
    "    features = []\n",
    "    for trial in params.n_trial_for_calc:\n",
    "        feature_files_trial = list(\n",
    "            Path(f\"{params.feature_3d_directory}\").glob(f\"t{trial:03}*.npy\")\n",
    "        )\n",
    "        feature_files_trial.sort()\n",
    "        for rep_path in feature_files_trial:\n",
    "            features_per_rep = np.load(rep_path)\n",
    "            max_distance = np.max(np.linalg.norm(features_per_rep, axis=1))\n",
    "            if max_distance > params.cutoff:\n",
    "                continue\n",
    "            else:\n",
    "                features.append(features_per_rep)\n",
    "    features_concat = np.concatenate(features, axis=0)\n",
    "    \n",
    "    # load clustering obj\n",
    "    clustering_model_pkl = f\"{params.output_directory}/cluster_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "    if not Path(clustering_model_pkl).exists():\n",
    "        params.logger.info(\n",
    "            f\"Clustering obj for n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping. First, run clustering.\"\n",
    "        )\n",
    "        return\n",
    "    with open(clustering_model_pkl, \"rb\") as f:\n",
    "        clustering_result = pickle.load(f)\n",
    "        dtrajs = clustering_result[\"trjs\"]\n",
    "        dtrajs_concat = np.concatenate(dtrajs, axis=0)\n",
    "\n",
    "    # load msm\n",
    "    msm_model_pkl = f\"{params.output_directory}/MSM_objs/3d_n_clusters{n_clusters}_cut{params.cutoff}.pkl\"\n",
    "    if not Path(msm_model_pkl).exists():\n",
    "        params.logger.info(\n",
    "            f\"MSM obj for n_clusters={n_clusters} cutoff={params.cutoff} does not exist. Skipping.\"\n",
    "            f\"First, run MSM.\"\n",
    "        )\n",
    "        return\n",
    "    with open(msm_model_pkl, \"rb\") as f:\n",
    "        msm_result = pickle.load(f)\n",
    "    \n",
    "    # find weights for each snapshot in trajectory\n",
    "    msm_model = msm_result[lag]\n",
    "    weights = msm_model.prior.compute_trajectory_weights(dtrajs_concat)[0]\n",
    "    \n",
    "    # plot\n",
    "    feature_dim = features[0].shape[-1]\n",
    "    \n",
    "    params.logger.info(\n",
    "        f\"Starting plotting 2D-FEL for n_clusters={n_clusters} lag={lag}\"\n",
    "    )\n",
    "    for first_dim in range(feature_dim):\n",
    "        for second_dim in range(first_dim+1, feature_dim):\n",
    "            first_coord = coord_names[first_dim]\n",
    "            second_coord = coord_names[second_dim]\n",
    "\n",
    "            energy_2d_obj = deeptime.util.energy2d(\n",
    "                x=features_concat[:, first_dim],\n",
    "                y=features_concat[:, second_dim],\n",
    "                weights=weights,\n",
    "                bins=100,\n",
    "                kbt=-params._RT, # negative sign to cancel the sign of energy\n",
    "                shift_energy=True,\n",
    "            )\n",
    "\n",
    "            # plot\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            ax, contour, cbar = deeptime.plots.plot_energy2d(\n",
    "                energies=energy_2d_obj,\n",
    "                ax=None,\n",
    "                levels=100,\n",
    "                contourf_kws=dict(cmap='nipy_spectral'),\n",
    "                # contourf_kws=dict(cmap='nipy_spectral', vmin=0, vmax=20), # to limit the range of colorbar\n",
    "                cbar=True,\n",
    "                cbar_kws=None,\n",
    "                cbar_ax=None,\n",
    "            )\n",
    "            cbar.set_label(r\"$\\Delta W$ [kcal/mol]\")\n",
    "            plt.title(f\"n_clusters={n_clusters} lag={lag} dim={first_coord}-{second_coord}\")\n",
    "            plt.xlabel(f\"{first_coord} [nm]\")\n",
    "            plt.ylabel(f\"{second_coord} [nm]\")\n",
    "            # plt.xlim(-10, 10) # to limit the range of x-axis\n",
    "            # plt.ylim(-10, 10) # to limit the range of y-axis\n",
    "            plt.savefig(\n",
    "                f\"{params.output_directory}/images/fel_2d_n_clusters{n_clusters}_lag{lag}_dim={first_coord}-{second_coord}_cut{params.cutoff}.png\",\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.1,\n",
    "                dpi=300,\n",
    "            )\n",
    "            if params.show_picture:\n",
    "                plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "    params.logger.info(\n",
    "        f\"Finished plotting 2D-FEL for n_clusters={n_clusters} lag={lag}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "for n_clusters in n_clusters_3d_main:\n",
    "    for lag in lags_3d_main:\n",
    "        plot_fel_each_2d(params, n_clusters, lag, coord_names=[\"X\", \"Y\", \"Z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Bowman, G. R.; No, F.; Pande, V. S. An Introduction to Markov State Models and Their Application to Long Timescale Molecular Simulation; Advances in Experimental Medicine and Biology; Springer Dordrecht, 2014.\n",
    "2. Husic, B. E.; Pande, V. S. Markov State Models: From an Art to a Science. J. Am. Chem. Soc. 2018, 140 (7), 2386 2396\n",
    "3. No, F.; Rosta, E. Markov Models of Molecular Kinetics. J. Chem. Phys. 2019, 151 (19), 190401,\n",
    "4. Hoffmann, M.; Scherer, M.; Hempel, T.; Mardt, A.; de Silva, B.; Husic, B. E.; Klus, S.; Wu, H.; Kutz, N.; Brunton, S. L.Deeptime: a Python library for machine learning dynamical models from time series data. Mach. Learn.: Sci. Technol. 2022, 3, 015009,\n",
    "5. Arthur, D.; Vassilvitskii, S. k-means++: the advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007 , 2007.\n",
    "6. Doudou, S.; Burton, N. A.; Henchman, R. H. Standard Free Energy of Binding from a One-Dimensional Potential of Mean Force. J. Chem. Theory Comput. 2009, 5 (4), 909 918,\n",
    "7. Buch, I.; Giorgino, T.; De Fabritiis, G. Complete reconstruction of an enzyme-inhibitor binding process by molecular dynamics simulations. Proc. Natl. Acad. Sci. U.S.A. 2011, 108 (25), 10184 10189,\n",
    "8.  Tran, D. P.; Takemura, K.; Kuwata, K.; Kitao, A. Protein-Ligand Dissociation Simulated by Parallel Cascade Selection Molecular Dynamics. J. Chem. Theory Comput. 2018, 14 (1), 404417.\n",
    "9.  Tran, D. P.; Kitao, A. Dissociation Process of a MDM2/p53 Complex Investigated by Parallel Cascade Selection Molecular Dynamics and the Markov State Model. J. Phys. Chem. B 2019, 123(11), 24692478\n",
    "10. Hata, H., Phuoc Tran, D., Marzouk Sobeh, M., & Kitao, A.Binding free energy of protein/ligand complexes calculated using dissociation Parallel Cascade Selection Molecular Dynamics and Markov state model. Biophys. Physicobiol. 2021, 18, 305316\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
